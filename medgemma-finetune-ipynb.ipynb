{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:33:42.930505Z","iopub.execute_input":"2025-07-21T19:33:42.930820Z","iopub.status.idle":"2025-07-21T19:33:42.934857Z","shell.execute_reply.started":"2025-07-21T19:33:42.930797Z","shell.execute_reply":"2025-07-21T19:33:42.934047Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# --- ورود به حساب کاربری Hugging Face ---\n# این کد را پس از تنظیم 'HF_TOKEN' در Kaggle Secrets، در یک سلول جداگانه اجرا کنید.\nfrom huggingface_hub import login\nimport os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"HF_TOKEN\")\ntry:\n    login(token=secret_value)\n    print(\"\\nSuccessfully logged in to Hugging Face using Kaggle Secret.\")\nexcept:\n    print(\"\\nError: HF_TOKEN Kaggle Secret not found. Please ensure it's created and attached.\")\n    print(\"You can try manual login by uncommenting the line below and running again.\")\n    # login() # uncomment this line to try manual login if secret method fails\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:26:22.832230Z","iopub.execute_input":"2025-07-21T19:26:22.832739Z","iopub.status.idle":"2025-07-21T19:26:23.560805Z","shell.execute_reply.started":"2025-07-21T19:26:22.832714Z","shell.execute_reply":"2025-07-21T19:26:23.560051Z"}},"outputs":[{"name":"stdout","text":"\nSuccessfully logged in to Hugging Face using Kaggle Secret.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\n\nprint(\"Loading the gaokerena/MF3QA dataset from Hugging Face Hub...\")\ntry:\n    dataset = load_dataset(\"gaokerena/MF3QA\")\n    print(\"Dataset loaded successfully.\")\n    print(\"\\nDataset structure:\")\n    print(dataset)\n    \n    if 'train' in dataset:\n        train_dataset = dataset['train']\n    else:\n        train_dataset = dataset[list(dataset.keys())[0]]\n\n    print(f\"\\nNumber of examples in the training split: {len(train_dataset)}\")\n    print(\"\\nFirst 5 raw examples from the dataset:\")\n    for i in range(min(5, len(train_dataset))):\n        print(f\"--- Example {i+1} ---\")\n        # *** اصلاح شده: استفاده از 'Question' و 'Answer' با حرف بزرگ ***\n        print(f\"Question: {train_dataset[i]['Question']}\")\n        print(f\"Answer: {train_dataset[i]['Answer']}\")\n\n    # --- فرمت بندی دیتاست برای Instruction Tuning ---\n    def format_example(example):\n        # *** اصلاح شده: استفاده از 'Question' و 'Answer' با حرف بزرگ ***\n        question = str(example.get('Question', '')).strip()\n        answer = str(example.get('Answer', '')).strip()\n\n        formatted_text = f\"سوال: {question}\\nپاسخ: {answer}\"\n        return {\"text\": formatted_text}\n\n    print(\"\\nFormatting the dataset into 'text' column...\")\n    # remove_columns باید با نام ستون های اصلی دیتاست مطابقت داشته باشد.\n    formatted_dataset = train_dataset.map(format_example, remove_columns=train_dataset.column_names)\n    \n    print(\"\\nFirst 3 formatted examples:\")\n    for i in range(min(3, len(formatted_dataset))):\n        print(f\"--- Formatted Example {i+1} ---\")\n        print(formatted_dataset[i]['text'])\n\n    print(\"\\nDataset preparation for fine-tuning is complete. Ready for model loading and tokenization.\")\n\nexcept Exception as e:\n    print(f\"An error occurred during dataset loading or preparation: {e}\")\n    print(\"Please ensure you have successfully logged in to Hugging Face and your internet connection is stable.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:26:23.562208Z","iopub.execute_input":"2025-07-21T19:26:23.562488Z","iopub.status.idle":"2025-07-21T19:26:29.212252Z","shell.execute_reply.started":"2025-07-21T19:26:23.562469Z","shell.execute_reply":"2025-07-21T19:26:29.211675Z"}},"outputs":[{"name":"stdout","text":"Loading the gaokerena/MF3QA dataset from Hugging Face Hub...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4279f1c51ab472a80b30c400756df1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3ba09af9e794b9780fb8068000564cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev-00000-of-00001.parquet:   0%|          | 0.00/1.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1998af1a1dc44e48b85c43f5418be2a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/796k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"490714c5861d475787c0a100214d9371"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c42dec8b194440aaabc5f0540718a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37024fb19e3f4faba8f5b09107ecf262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051238d168b84d83a051329005ed5038"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded successfully.\n\nDataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['Question', 'Answer', 'Source'],\n        num_rows: 20000\n    })\n    dev: Dataset({\n        features: ['Question', 'Answer', 'Source'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['Question', 'Answer', 'Source'],\n        num_rows: 2000\n    })\n})\n\nNumber of examples in the training split: 20000\n\nFirst 5 raw examples from the dataset:\n--- Example 1 ---\nQuestion: ۱ماهپیش از خواب پریدم از ترس شدید بعداز اون ترس،فشارم بالا میرفت ۱۴ ۱۵ با سر درد و درد قفسه سینه همراه بود با پرانول کنترش میکردم ولی الان ۲ ۳ روزه فشارم میاد پاین ۸ ۹ روی ۳ ۴ وقتایی که میرم پیاده روی یا کلا فعالیتی دارم بعدش ضربانم تا ۶ ۷ ساعت حتی بیشتر تنده و کند و طبیعی نمیشه خستهه شدم دیگهمیترسم اتفاقی برام بیوفته فشار ۸ ۹ روری ۴ ۵ خطرناکه؟\nAnswer: تجربه‌ای که شرح داده‌اید، نشان‌دهنده‌ی رخدادهای فیزیولوژیک و احتمالاً پاتولوژیک در بدن شما است. ترس شدید و پرش از خواب می‌تواند منجر به افزایش موقت فشار خون و تپش قلب شود، که این واکنشی طبیعی است. با این حال، ادامه‌ی این علائم و تغییرات فشار خون از بالا به پایین نیازمند بررسی بیشتر است. فشار خون شما که گاهی به 8/9 می‌رسد و ضربان قلب بالا پس از فعالیت، ممکن است نشانه‌ای از اختلال در تنظیم فشار خون یا مشکلات قلبی باشد. مراجعه به پزشک، انجام آزمایش‌های تخصصی قلب و عروق و شاید مشاوره با یک روانپزشک یا روانشناس برای مدیریت استرس و ترس شدید پیشنهاد می‌شود. اطمینان از کنترل درست فشار خون با دارو و برنامه‌ریزی برای فعالیت بدنی مناسب و مداوم اهمیت دارد. وضعیت فشار 8/9 روی 4/5 که تجربه می‌کنید، لازم است که به صورت جدی توسط یک متخصص قلب مورد ارزیابی قرار گیرد تا از خطرات احتمالی جلوگیری شود.\n--- Example 2 ---\nQuestion: پسر من ۳ماهه و ۸روز دارد و وزن او ۷.۳۰۰ است شکمش ورم دارد و همش میخواد انگشت شصتش بمکد و دیر باد گلوش بیرون میاد و صبح ها با بینی گرفته بیدار میشه و همش میخواد شیر بخوره و گاهی از شیر خوردن امتناع میکنه و سر او خیلی داغه و همش عرق میکنه چند وقتی هیت در کنار شیر خورم هر از گاهی شیر خشک سوپرامیل هم بهش میدم ممکنه میشه راهنماییم کنید؟\nAnswer: بر اساس توضیحاتی که ارائه دادید، نشانه‌هایی که فرزندتان نشان می‌دهد مانند مکیدن انگشت، باد گلوی دیر هضم، بینی گرفته در صبح‌ها، و دمای بدن بالا می‌توانند نشانه‌هایی از رفلاکس پنهان باشند. رفلاکس در نوزادان زمانی رخ می‌دهد که شیر یا فرمولا مصرفی به مری برگردد، که می‌تواند به علائمی نظیر تحریک‌پذیری و نشانه‌هایی که شما توضیح داده‌اید منجر شود. تغییرات در رژیم غذایی مادر در صورت شیردهی، استفاده از شیر خشک هایی که کمتر حساسیت‌زا هستند، و نگه داشتن کودک در وضعیت نیمه نشسته پس از غذا می‌توانند کمک‌کننده باشند. همچنین، توصیه می‌شود جهت ارزیابی دقیق‌تر و درمان، با پزشک کودکان مشورت نمایید.\n--- Example 3 ---\nQuestion: بیماری ra دارم دکتر برام پردنیزلون ۵ و سولفاسالازین ۵۰۰ تجویز کرده بهمراه اسید فولیک یک هفته هست که از داروها استفاده می کنم اما دو روزی که کلیه سمت راستم درد میکنه می خواستم بدونم از عوارض سولفاسالازین و اینکه اگه هست باید قطع کنم و جایگزین دارو چی استفاده کنم یا نه از دارو دیگه تا هفته اینده به پزشکم درستی ندارم لطفا راهنمایی بفرمایین؟\nAnswer: درد کلیه پس از مصرف دارو در برخی موارد می‌تواند نشان‌دهنده واکنش بدن به دارو باشد. سولفاسالازین، که برای درمان روماتوئید آرتریت (RA) تجویز می‌شود، ممکن است باعث افزایش خطر بروز مشکلات کلیوی شود، اگرچه این عارضه نسبتا نادر است. علائمی مانند درد کلیه نیازمند توجه پزشکی فوری است تا از عوارض جدی‌تر پیشگیری شود. مناسب است که فوراً با پزشک خود تماس بگیرید و در مورد علائم خود اطلاع دهید. ممکن است پزشک تصمیم بگیرد که آزمایش‌های بیشتری انجام دهد یا داروی جایگزین تجویز کند. توصیه می‌شود تا زمان مشورت با پزشک، دارو را بدون توصیه‌ی پزشک قطع نکنید.\n--- Example 4 ---\nQuestion: سلام. . دوهفته پیش قبل از پریودی تا چند روز درد در ان ناحیه که توده ای حس میشد حس کردم. سونوگرافی اول که رفتم گفتن مشکوک هست و باید نمونه برداری بشه. منم رفتم ماموگرافی اما دوباره گفتن مجددا باید سونو بشم. میشه بفرمایید مشکلی هست؟\nAnswer: بافت سینه متراکم هتروژن به این معنی است که بافت سینه شامل ترکیبات متنوعی از بافت چربی و بافت غددی-فیبری است که می‌تواند تفسیر تصاویر ماموگرافی را دشوار سازد. درد و احساس وجود توده می‌تواند ناشی از تغییرات هورمونی قبل از پریود یا سایر عوامل باشد، اما توصیه عمومی این است که هرگونه مشکوکی باید با دقت بررسی شود. \n\t\tدر مواردی که سونوگرافی یا ماموگرافی نتایج مشکوک نشان می‌دهد، انجام مجدد سونوگرافی یا نمونه‌برداری (بیوپسی) توصیه می‌شود تا ماهیت دقیق توده یا تغییرات بافتی مشخص شود. گاهی اوقات، درخواست برای سونوگرافی مجدد به دلیل نیاز به تصاویر واضح‌تر یا ارزیابی بیشتر است. مهم است که دستورالعمل‌های پزشکی را دقیقاً دنبال کرده و جهت کسب اطمینان بیشتر و رد هرگونه بیماری جدی‌تر، پیگیری‌های لازم انجام شود.\n--- Example 5 ---\nQuestion: سلام وقت بخیر دندون عقلم س ساعت پیش کشیدم ولی همچنان نمیشه گاز استریل برداشت و خونریزی داره باید چکار کنم؟\nAnswer: سلام دوست عزیز ، معمولا خون آبه هست ، سعی کنید قورت بدید و اصلا تف نکنید . اگر بعد از فشار مداوم یکی دو ساعت ، خون نه خون آبه در دهانتون پر میشه، به دندانپزشک مراجعه کنید که براتون بخیه بزنن یا یک آمپول ترانگزامیک اسید بخرید روی گاز بریزید بزارید روی ناحیه خونریزی .\n\nFormatting the dataset into 'text' column...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a7682c6d0b44e36afa05f568b76f60a"}},"metadata":{}},{"name":"stdout","text":"\nFirst 3 formatted examples:\n--- Formatted Example 1 ---\nسوال: ۱ماهپیش از خواب پریدم از ترس شدید بعداز اون ترس،فشارم بالا میرفت ۱۴ ۱۵ با سر درد و درد قفسه سینه همراه بود با پرانول کنترش میکردم ولی الان ۲ ۳ روزه فشارم میاد پاین ۸ ۹ روی ۳ ۴ وقتایی که میرم پیاده روی یا کلا فعالیتی دارم بعدش ضربانم تا ۶ ۷ ساعت حتی بیشتر تنده و کند و طبیعی نمیشه خستهه شدم دیگهمیترسم اتفاقی برام بیوفته فشار ۸ ۹ روری ۴ ۵ خطرناکه؟\nپاسخ: تجربه‌ای که شرح داده‌اید، نشان‌دهنده‌ی رخدادهای فیزیولوژیک و احتمالاً پاتولوژیک در بدن شما است. ترس شدید و پرش از خواب می‌تواند منجر به افزایش موقت فشار خون و تپش قلب شود، که این واکنشی طبیعی است. با این حال، ادامه‌ی این علائم و تغییرات فشار خون از بالا به پایین نیازمند بررسی بیشتر است. فشار خون شما که گاهی به 8/9 می‌رسد و ضربان قلب بالا پس از فعالیت، ممکن است نشانه‌ای از اختلال در تنظیم فشار خون یا مشکلات قلبی باشد. مراجعه به پزشک، انجام آزمایش‌های تخصصی قلب و عروق و شاید مشاوره با یک روانپزشک یا روانشناس برای مدیریت استرس و ترس شدید پیشنهاد می‌شود. اطمینان از کنترل درست فشار خون با دارو و برنامه‌ریزی برای فعالیت بدنی مناسب و مداوم اهمیت دارد. وضعیت فشار 8/9 روی 4/5 که تجربه می‌کنید، لازم است که به صورت جدی توسط یک متخصص قلب مورد ارزیابی قرار گیرد تا از خطرات احتمالی جلوگیری شود.\n--- Formatted Example 2 ---\nسوال: پسر من ۳ماهه و ۸روز دارد و وزن او ۷.۳۰۰ است شکمش ورم دارد و همش میخواد انگشت شصتش بمکد و دیر باد گلوش بیرون میاد و صبح ها با بینی گرفته بیدار میشه و همش میخواد شیر بخوره و گاهی از شیر خوردن امتناع میکنه و سر او خیلی داغه و همش عرق میکنه چند وقتی هیت در کنار شیر خورم هر از گاهی شیر خشک سوپرامیل هم بهش میدم ممکنه میشه راهنماییم کنید؟\nپاسخ: بر اساس توضیحاتی که ارائه دادید، نشانه‌هایی که فرزندتان نشان می‌دهد مانند مکیدن انگشت، باد گلوی دیر هضم، بینی گرفته در صبح‌ها، و دمای بدن بالا می‌توانند نشانه‌هایی از رفلاکس پنهان باشند. رفلاکس در نوزادان زمانی رخ می‌دهد که شیر یا فرمولا مصرفی به مری برگردد، که می‌تواند به علائمی نظیر تحریک‌پذیری و نشانه‌هایی که شما توضیح داده‌اید منجر شود. تغییرات در رژیم غذایی مادر در صورت شیردهی، استفاده از شیر خشک هایی که کمتر حساسیت‌زا هستند، و نگه داشتن کودک در وضعیت نیمه نشسته پس از غذا می‌توانند کمک‌کننده باشند. همچنین، توصیه می‌شود جهت ارزیابی دقیق‌تر و درمان، با پزشک کودکان مشورت نمایید.\n--- Formatted Example 3 ---\nسوال: بیماری ra دارم دکتر برام پردنیزلون ۵ و سولفاسالازین ۵۰۰ تجویز کرده بهمراه اسید فولیک یک هفته هست که از داروها استفاده می کنم اما دو روزی که کلیه سمت راستم درد میکنه می خواستم بدونم از عوارض سولفاسالازین و اینکه اگه هست باید قطع کنم و جایگزین دارو چی استفاده کنم یا نه از دارو دیگه تا هفته اینده به پزشکم درستی ندارم لطفا راهنمایی بفرمایین؟\nپاسخ: درد کلیه پس از مصرف دارو در برخی موارد می‌تواند نشان‌دهنده واکنش بدن به دارو باشد. سولفاسالازین، که برای درمان روماتوئید آرتریت (RA) تجویز می‌شود، ممکن است باعث افزایش خطر بروز مشکلات کلیوی شود، اگرچه این عارضه نسبتا نادر است. علائمی مانند درد کلیه نیازمند توجه پزشکی فوری است تا از عوارض جدی‌تر پیشگیری شود. مناسب است که فوراً با پزشک خود تماس بگیرید و در مورد علائم خود اطلاع دهید. ممکن است پزشک تصمیم بگیرد که آزمایش‌های بیشتری انجام دهد یا داروی جایگزین تجویز کند. توصیه می‌شود تا زمان مشورت با پزشک، دارو را بدون توصیه‌ی پزشک قطع نکنید.\n\nDataset preparation for fine-tuning is complete. Ready for model loading and tokenization.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"Ensuring all necessary libraries are installed...\")\n!pip install -q -U transformers peft trl bitsandbytes scipy datasets\n!pip install -q -U \"huggingface_hub[cli]\"\n# !git config --global user.name \"lbehradl\"\n# !git clone https://github.com/unslothai/unsloth.git\n# !pip install -q -U ./unscdloth\n# !pip install -q -U unsloth_zoo\nprint(\"Libraries installation/update complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:30:19.252737Z","iopub.execute_input":"2025-07-21T19:30:19.253403Z","iopub.status.idle":"2025-07-21T19:30:27.162627Z","shell.execute_reply.started":"2025-07-21T19:30:19.253377Z","shell.execute_reply":"2025-07-21T19:30:27.161836Z"}},"outputs":[{"name":"stdout","text":"Ensuring all necessary libraries are installed...\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth-zoo 2025.7.8 requires datasets<4.0.0,>=3.4.1, but you have datasets 4.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mLibraries installation/update complete.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install -U \"scipy==1.11.4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:31:38.914332Z","iopub.execute_input":"2025-07-21T19:31:38.914814Z","iopub.status.idle":"2025-07-21T19:31:46.536579Z","shell.execute_reply.started":"2025-07-21T19:31:38.914790Z","shell.execute_reply":"2025-07-21T19:31:46.535627Z"}},"outputs":[{"name":"stdout","text":"Collecting scipy==1.11.4\n  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from scipy==1.11.4) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<1.28.0,>=1.21.6->scipy==1.11.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<1.28.0,>=1.21.6->scipy==1.11.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<1.28.0,>=1.21.6->scipy==1.11.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<1.28.0,>=1.21.6->scipy==1.11.4) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<1.28.0,>=1.21.6->scipy==1.11.4) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<1.28.0,>=1.21.6->scipy==1.11.4) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<1.28.0,>=1.21.6->scipy==1.11.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<1.28.0,>=1.21.6->scipy==1.11.4) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<1.28.0,>=1.21.6->scipy==1.11.4) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<1.28.0,>=1.21.6->scipy==1.11.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<1.28.0,>=1.21.6->scipy==1.11.4) (2024.2.0)\nDownloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.0\n    Uninstalling scipy-1.16.0:\n      Successfully uninstalled scipy-1.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.11.4\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import unsloth\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\ntry:\n    from unsloth import FastLanguageModel\n    print(\"Unsloth detected. Using FastLanguageModel for optimized loading.\")\n    USE_UNSLOTH = True\nexcept ImportError:\n    print(\"Unsloth not found. Falling back to standard Hugging Face loading.\")\n    USE_UNSLOTH = False\nprint('imported')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:35:13.613634Z","iopub.execute_input":"2025-07-21T19:35:13.614387Z","iopub.status.idle":"2025-07-21T19:35:13.619457Z","shell.execute_reply.started":"2025-07-21T19:35:13.614357Z","shell.execute_reply":"2025-07-21T19:35:13.618517Z"}},"outputs":[{"name":"stdout","text":"Unsloth detected. Using FastLanguageModel for optimized loading.\nimported\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"model_name = \"google/medgemma-4b-it\" # یا \"google/medgemma-27b\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\", # نوع کوانتیزیشن\n    bnb_4bit_compute_dtype=torch.bfloat16, # نوع داده برای محاسبات\n    bnb_4bit_use_double_quant=True, # استفاده از کوانتیزیشن دوگانه برای کاهش بیشتر حافظه\n)\n\n# --- بارگذاری مدل و توکنایزر ---\n# اگر Unsloth در دسترس باشد، از آن برای بارگذاری بهینه استفاده می‌کنیم.\nif USE_UNSLOTH:\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=model_name,\n        max_seq_length=1024, # حداکثر طول دنباله (sequence length) برای ورودی های مدل\n                               # این مقدار را می توانید بر اساس طول سوالات و پاسخ های خود تنظیم کنید.\n                               # 2048 یک مقدار رایج و مناسب برای شروع است.\n        dtype=None, # None به unsloth اجازه می دهد بهترین dtype را انتخاب کند (معمولا bfloat16)\n        load_in_4bit=True, # فعال کردن کوانتیزیشن 4-bit\n    )\nelse:\n    # بارگذاری توکنایزر\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    # اطمینان از تنظیم pad_token برای توکنایزر\n    # این برای مدل های decoder-only مهم است.\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token # End-of-sequence token به عنوان pad_token\n\n    # بارگذاری مدل با تنظیمات کوانتیزیشن\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\", # برای توزیع مدل روی GPU های موجود\n        torch_dtype=torch.bfloat16, # استفاده از bfloat16 برای محاسبات\n    )\n    # آماده سازی مدل برای آموزش LoRA با کوانتیزیشن 4-bit\n    model = prepare_model_for_kbit_training(model)\n\nprint(f\"\\nModel '{model_name}' and Tokenizer loaded successfully.\")\nprint(\"\\nModel structure (first few layers):\")\nprint(model) # نمایش ساختار مدل\n\n# --- تنظیمات توکنایزر برای آموزش ---\n# این تنظیمات برای اطمینان از اینکه توکنایزر به درستی برای آموزش آماده است، ضروری است.\ntokenizer.padding_side = \"right\" # پدینگ از سمت راست (برای مدل های decoder-only توصیه می شود)\n\nprint(\"\\nTokenizer padding side set to 'right'.\")\nprint(\"Ready for LoRA configuration and training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:35:21.417636Z","iopub.execute_input":"2025-07-21T19:35:21.417907Z","iopub.status.idle":"2025-07-21T19:36:00.864544Z","shell.execute_reply.started":"2025-07-21T19:35:21.417888Z","shell.execute_reply":"2025-07-21T19:36:00.863750Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.7.6: Fast Gemma3 patching. Transformers: 4.53.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5398c2269343afb2ff69276ad6d7ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9366f13d667b49e2893ffd0bf01f5a0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"134bdc6b5c0b43448081d2800df3bd48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bc03fba24bd481bbf484881b370fd33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feb42fb5269f44568590658e9df022d0"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"507d09b5b286449183ca1e5d949e85bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d529f011ffb41989629f7566669613f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfdcd87aaf584461a3790c08a520a8c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"688788731fc14ff0a30f5ef281708ddf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4241dc5242774d9490ac54b5a6279d1b"}},"metadata":{}},{"name":"stdout","text":"\nModel 'google/medgemma-4b-it' and Tokenizer loaded successfully.\n\nModel structure (first few layers):\nGemma3ForConditionalGeneration(\n  (model): Gemma3Model(\n    (vision_tower): SiglipVisionModel(\n      (vision_model): SiglipVisionTransformer(\n        (embeddings): SiglipVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n          (position_embedding): Embedding(4096, 1152)\n        )\n        (encoder): SiglipEncoder(\n          (layers): ModuleList(\n            (0-15): 16 x SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n            (16): SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n            (17-18): 2 x SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n            (19-20): 2 x SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n            (21-22): 2 x SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n            (23): SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n            (24-25): 2 x SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n            (26): SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n      )\n    )\n    (multi_modal_projector): Gemma3MultiModalProjector(\n      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n    )\n    (language_model): Gemma3TextModel(\n      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n      (layers): ModuleList(\n        (0): Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n        (1): Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n        (2): Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n        (3): Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n        (4-5): 2 x Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n        (6): Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n        (7-9): 3 x Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n        (10-33): 24 x Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n      )\n      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n      (rotary_emb): Gemma3RotaryEmbedding()\n      (rotary_emb_local): Gemma3RotaryEmbedding()\n    )\n  )\n  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n)\n\nTokenizer padding side set to 'right'.\nReady for LoRA configuration and training.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# --- مرحله 4: پیکربندی LoRA و Trainer ---\n# این کد را در یک سلول جدید در Kaggle Notebook خودتان اجرا کنید.\n\n# مطمئن شوید که 'model' و 'tokenizer' از مرحله قبل در دسترس هستند.\n# همچنین 'formatted_dataset' که در مرحله 2 آماده کردیم.\n\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer # SFTConfig در اینجا استفاده نمی شود، از TrainingArguments استفاده می کنیم.\n\nprint(\"Configuring LoRA and Training Arguments for GPU...\")\n\n# --- 4.1: پیکربندی LoRA (Low-Rank Adaptation) ---\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# --- اعمال پیکربندی LoRA به مدل ---\n# اگر از Unsloth استفاده می کنید، FastLanguageModel.from_pretrained ممکن است این کار را به صورت خودکار انجام دهد.\n# اگر خطای \"Adapter with name default already exists\" گرفتید، این خط را کامنت کنید.\nmodel = get_peft_model(model, lora_config)\n\nprint(\"LoRA adapters attached to the model.\")\n\n# --- 4.2: تنظیمات آموزش (TrainingArguments) ---\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=2, # تعداد اپوک ها برای آموزش.\n    per_device_train_batch_size=4, # *** اصلاح شده: افزایش بچ سایز به 4 برای استفاده بیشتر از GPU ***\n    gradient_accumulation_steps=2, # *** اصلاح شده: کاهش گام های انباشت گرادیان برای حفظ effective batch size (4 * 2 = 8) ***\n    optim=\"paged_adamw_8bit\", # بهینه ساز برای QLoRA در GPU.\n    save_steps=500, # تعداد گام ها برای ذخیره چک پوینت مدل.\n    logging_steps=50, # تعداد گام ها برای لاگ کردن اطلاعات آموزش (loss و غیره).\n    learning_rate=2e-4, # نرخ یادگیری برای بهینه ساز.\n    fp16=False, # استفاده از float32 (با تنظیم هر دو fp16 و bf16 به False).\n    bf16=False, # استفاده از float32 (با تنظیم هر دو fp16 و bf16 به False).\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\",\n    disable_tqdm=False, # فعال کردن نوار پیشرفت برای مشاهده وضعیت.\n    torch_compile=False, # غیرفعال کردن torch.compile برای سازگاری با P100.\n    # اگر می خواهید مدل را در Hugging Face Hub آپلود کنید، این پارامترها را فعال کنید:\n    # push_to_hub=True,\n    # hub_model_id=\"your-username/medgemma-fa-medical-qa\",\n    # hub_private_repo=False,\n    # hub_strategy=\"every_save\",\n)\n\n# --- 4.3: آماده سازی SFTTrainer ---\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=formatted_dataset,\n    peft_config=lora_config,\n    args=training_arguments,\n    tokenizer=tokenizer,\n    max_seq_length=512, # طول دنباله برای سرعت بیشتر.\n    dataset_text_field=\"text\",\n    packing=False,\n)\n\nprint(\"\\nLoRA and Training Arguments configured successfully.\")\nprint(\"SFTTrainer initialized. Ready to start training!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:46:59.475040Z","iopub.execute_input":"2025-07-21T19:46:59.475824Z","iopub.status.idle":"2025-07-21T19:47:05.498968Z","shell.execute_reply.started":"2025-07-21T19:46:59.475797Z","shell.execute_reply":"2025-07-21T19:47:05.498348Z"}},"outputs":[{"name":"stdout","text":"Configuring LoRA and Training Arguments for GPU...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"LoRA adapters attached to the model.\nUnsloth: Switching to float32 training since model cannot work with float16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1a6e48d18f4677a9b69b64439f9db4"}},"metadata":{}},{"name":"stdout","text":"\nLoRA and Training Arguments configured successfully.\nSFTTrainer initialized. Ready to start training!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# --- مرحله 5: اجرای فاین تیونینگ ---\n# این کد را در یک سلول جدید در Kaggle Notebook خودتان اجرا کنید.\n\n# مطمئن شوید که 'trainer' از مرحله 4 در دسترس است.\n\nprint(\"Starting the fine-tuning process. This may take a while depending on your GPU and dataset size.\")\nprint(\"The training progress will be logged based on 'logging_steps' defined in TrainingArguments.\")\n\n# شروع فرایند آموزش\ntrainer.train()\n\nprint(\"\\nFine-tuning process completed successfully!\")\n\n# --- مرحله 6: ذخیره مدل فاین تیون شده ---\n# پس از اتمام آموزش، مدل فاین تیون شده را ذخیره می کنیم.\n# این شامل وزن های LoRA (آداپتورها) است که می تواند با مدل پایه ادغام شود.\n\n# مسیر ذخیره سازی مدل\n# این مسیر باید با output_dir در TrainingArguments مطابقت داشته باشد.\noutput_dir_model = \"./results/final_model\"\n\nprint(f\"\\nSaving the fine-tuned model to: {output_dir_model}\")\n# ذخیره آداپتورهای LoRA\ntrainer.model.save_pretrained(output_dir_model)\nprint(\"LoRA adapters saved.\")\n\n# ذخیره توکنایزر\ntokenizer.save_pretrained(output_dir_model)\nprint(\"Tokenizer saved.\")\n\n# --- ادغام آداپتورهای LoRA با مدل پایه (اختیاری و برای استقرار نهایی) ---\n# این بخش نیاز به RAM GPU بیشتری دارد. اگر در آینده قصد استقرار مدل کامل را دارید،\n# می توانید این بخش را فعال کنید. فعلاً کامنت شده است.\n# اگر از Unsloth استفاده می کنید، می توانید از تابع FastLanguageModel.save_pretrained_merged استفاده کنید.\n# if USE_UNSLOTH:\n#     print(\"\\nMerging LoRA adapters with the base model and saving the merged model...\")\n#     # save_method=\"merged_16bit\" یا \"merged_4bit\" بسته به نیاز شما\n#     trainer.model.save_pretrained_merged(output_dir_model, tokenizer, save_method = \"merged_4bit\")\n#     print(\"Merged model saved.\")\n# else:\n#     print(\"\\nTo merge LoRA adapters with the base model (without Unsloth), run the following (requires more RAM):\")\n#     print(\"from peft import PeftModel\")\n#     print(\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n#     print(f\"base_model_loaded = AutoModelForCausalLM.from_pretrained('{model_name}', torch_dtype=torch.float32, device_map='auto')\")\n#     print(f\"merged_model = PeftModel.from_pretrained(base_model_loaded, '{output_dir_model}')\")\n#     print(\"merged_model = merged_model.merge_and_unload()\")\n#     print(\"merged_model.save_pretrained('./results/merged_model')\")\n#     print(\"tokenizer.save_pretrained('./results/merged_model')\")\n\n\nprint(\"\\nModel fine-tuning and saving process complete. You can now load this model for inference.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:36:19.643377Z","iopub.execute_input":"2025-07-21T19:36:19.644172Z","iopub.status.idle":"2025-07-21T19:46:43.370442Z","shell.execute_reply.started":"2025-07-21T19:36:19.644148Z","shell.execute_reply":"2025-07-21T19:46:43.369260Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 20,000 | Num Epochs = 3 | Total steps = 3,750\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n \"-____-\"     Trainable parameters = 32,788,480 of 4,332,867,952 (0.76% trained)\n","output_type":"stream"},{"name":"stdout","text":"Starting the fine-tuning process. This may take a while depending on your GPU and dataset size.\nThe training progress will be logged based on 'logging_steps' defined in TrainingArguments.\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='51' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  51/3750 08:50 < 11:07:23, 0.09 it/s, Epoch 0.04/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1473994404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# شروع فرایند آموزش\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFine-tuning process completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":19}]}