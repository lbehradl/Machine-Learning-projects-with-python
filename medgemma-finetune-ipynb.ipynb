{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- ูุฑูุฏ ุจู ุญุณุงุจ ฺฉุงุฑุจุฑ Hugging Face ---\n# ุงู ฺฉุฏ ุฑุง ูพุณ ุงุฒ ุชูุธู 'HF_TOKEN' ุฏุฑ Kaggle Secretsุ ุฏุฑ ฺฉ ุณููู ุฌุฏุงฺฏุงูู ุงุฌุฑุง ฺฉูุฏ.\nfrom huggingface_hub import login\nimport os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"HF_TOKEN\")\ntry:\n    login(token=secret_value)\n    print(\"\\nSuccessfully logged in to Hugging Face using Kaggle Secret.\")\nexcept:\n    print(\"\\nError: HF_TOKEN Kaggle Secret not found. Please ensure it's created and attached.\")\n    print(\"You can try manual login by uncommenting the line below and running again.\")\n    # login() # uncomment this line to try manual login if secret method fails\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:13:26.241916Z","iopub.execute_input":"2025-07-21T19:13:26.242349Z","iopub.status.idle":"2025-07-21T19:13:26.810608Z","shell.execute_reply.started":"2025-07-21T19:13:26.242329Z","shell.execute_reply":"2025-07-21T19:13:26.809928Z"}},"outputs":[{"name":"stdout","text":"\nSuccessfully logged in to Hugging Face using Kaggle Secret.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\n\nprint(\"Loading the gaokerena/MF3QA dataset from Hugging Face Hub...\")\ntry:\n    dataset = load_dataset(\"gaokerena/MF3QA\")\n    print(\"Dataset loaded successfully.\")\n    print(\"\\nDataset structure:\")\n    print(dataset)\n    \n    if 'train' in dataset:\n        train_dataset = dataset['train']\n    else:\n        train_dataset = dataset[list(dataset.keys())[0]]\n\n    print(f\"\\nNumber of examples in the training split: {len(train_dataset)}\")\n    print(\"\\nFirst 5 raw examples from the dataset:\")\n    for i in range(min(5, len(train_dataset))):\n        print(f\"--- Example {i+1} ---\")\n        # *** ุงุตูุงุญ ุดุฏู: ุงุณุชูุงุฏู ุงุฒ 'Question' ู 'Answer' ุจุง ุญุฑู ุจุฒุฑฺฏ ***\n        print(f\"Question: {train_dataset[i]['Question']}\")\n        print(f\"Answer: {train_dataset[i]['Answer']}\")\n\n    # --- ูุฑูุช ุจูุฏ ุฏุชุงุณุช ุจุฑุง Instruction Tuning ---\n    def format_example(example):\n        # *** ุงุตูุงุญ ุดุฏู: ุงุณุชูุงุฏู ุงุฒ 'Question' ู 'Answer' ุจุง ุญุฑู ุจุฒุฑฺฏ ***\n        question = str(example.get('Question', '')).strip()\n        answer = str(example.get('Answer', '')).strip()\n\n        formatted_text = f\"ุณูุงู: {question}\\nูพุงุณุฎ: {answer}\"\n        return {\"text\": formatted_text}\n\n    print(\"\\nFormatting the dataset into 'text' column...\")\n    # remove_columns ุจุงุฏ ุจุง ูุงู ุณุชูู ูุง ุงุตู ุฏุชุงุณุช ูุทุงุจูุช ุฏุงุดุชู ุจุงุดุฏ.\n    formatted_dataset = train_dataset.map(format_example, remove_columns=train_dataset.column_names)\n    \n    print(\"\\nFirst 3 formatted examples:\")\n    for i in range(min(3, len(formatted_dataset))):\n        print(f\"--- Formatted Example {i+1} ---\")\n        print(formatted_dataset[i]['text'])\n\n    print(\"\\nDataset preparation for fine-tuning is complete. Ready for model loading and tokenization.\")\n\nexcept Exception as e:\n    print(f\"An error occurred during dataset loading or preparation: {e}\")\n    print(\"Please ensure you have successfully logged in to Hugging Face and your internet connection is stable.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:13:31.485071Z","iopub.execute_input":"2025-07-21T19:13:31.485820Z","iopub.status.idle":"2025-07-21T19:13:34.728679Z","shell.execute_reply.started":"2025-07-21T19:13:31.485795Z","shell.execute_reply":"2025-07-21T19:13:34.727808Z"}},"outputs":[{"name":"stdout","text":"Loading the gaokerena/MF3QA dataset from Hugging Face Hub...\nDataset loaded successfully.\n\nDataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['Question', 'Answer', 'Source'],\n        num_rows: 20000\n    })\n    dev: Dataset({\n        features: ['Question', 'Answer', 'Source'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['Question', 'Answer', 'Source'],\n        num_rows: 2000\n    })\n})\n\nNumber of examples in the training split: 20000\n\nFirst 5 raw examples from the dataset:\n--- Example 1 ---\nQuestion: ฑูุงููพุด ุงุฒ ุฎูุงุจ ูพุฑุฏู ุงุฒ ุชุฑุณ ุดุฏุฏ ุจุนุฏุงุฒ ุงูู ุชุฑุณุูุดุงุฑู ุจุงูุง ูุฑูุช ฑด ฑต ุจุง ุณุฑ ุฏุฑุฏ ู ุฏุฑุฏ ููุณู ุณูู ููุฑุงู ุจูุฏ ุจุง ูพุฑุงููู ฺฉูุชุฑุด ูฺฉุฑุฏู ูู ุงูุงู ฒ ณ ุฑูุฒู ูุดุงุฑู ูุงุฏ ูพุงู ธ น ุฑู ณ ด ููุชุง ฺฉู ูุฑู ูพุงุฏู ุฑู ุง ฺฉูุง ูุนุงูุช ุฏุงุฑู ุจุนุฏุด ุถุฑุจุงูู ุชุง ถ ท ุณุงุนุช ุญุช ุจุดุชุฑ ุชูุฏู ู ฺฉูุฏ ู ุทุจุน ููุดู ุฎุณุชูู ุดุฏู ุฏฺฏููุชุฑุณู ุงุชูุงู ุจุฑุงู ุจููุชู ูุดุงุฑ ธ น ุฑูุฑ ด ต ุฎุทุฑูุงฺฉูุ\nAnswer: ุชุฌุฑุจูโุง ฺฉู ุดุฑุญ ุฏุงุฏูโุงุฏุ ูุดุงูโุฏููุฏูโ ุฑุฎุฏุงุฏูุง ูุฒูููฺฺฉ ู ุงุญุชูุงูุงู ูพุงุชูููฺฺฉ ุฏุฑ ุจุฏู ุดูุง ุงุณุช. ุชุฑุณ ุดุฏุฏ ู ูพุฑุด ุงุฒ ุฎูุงุจ ูโุชูุงูุฏ ููุฌุฑ ุจู ุงูุฒุงุด ูููุช ูุดุงุฑ ุฎูู ู ุชูพุด ููุจ ุดูุฏุ ฺฉู ุงู ูุงฺฉูุด ุทุจุน ุงุณุช. ุจุง ุงู ุญุงูุ ุงุฏุงููโ ุงู ุนูุงุฆู ู ุชุบุฑุงุช ูุดุงุฑ ุฎูู ุงุฒ ุจุงูุง ุจู ูพุงู ูุงุฒููุฏ ุจุฑุฑุณ ุจุดุชุฑ ุงุณุช. ูุดุงุฑ ุฎูู ุดูุง ฺฉู ฺฏุงู ุจู 8/9 ูโุฑุณุฏ ู ุถุฑุจุงู ููุจ ุจุงูุง ูพุณ ุงุฒ ูุนุงูุชุ ููฺฉู ุงุณุช ูุดุงููโุง ุงุฒ ุงุฎุชูุงู ุฏุฑ ุชูุธู ูุดุงุฑ ุฎูู ุง ูุดฺฉูุงุช ููุจ ุจุงุดุฏ. ูุฑุงุฌุนู ุจู ูพุฒุดฺฉุ ุงูุฌุงู ุขุฒูุงุดโูุง ุชุฎุตุต ููุจ ู ุนุฑูู ู ุดุงุฏ ูุดุงูุฑู ุจุง ฺฉ ุฑูุงููพุฒุดฺฉ ุง ุฑูุงูุดูุงุณ ุจุฑุง ูุฏุฑุช ุงุณุชุฑุณ ู ุชุฑุณ ุดุฏุฏ ูพุดููุงุฏ ูโุดูุฏ. ุงุทููุงู ุงุฒ ฺฉูุชุฑู ุฏุฑุณุช ูุดุงุฑ ุฎูู ุจุง ุฏุงุฑู ู ุจุฑูุงููโุฑุฒ ุจุฑุง ูุนุงูุช ุจุฏู ููุงุณุจ ู ูุฏุงูู ุงููุช ุฏุงุฑุฏ. ูุถุนุช ูุดุงุฑ 8/9 ุฑู 4/5 ฺฉู ุชุฌุฑุจู ูโฺฉูุฏุ ูุงุฒู ุงุณุช ฺฉู ุจู ุตูุฑุช ุฌุฏ ุชูุณุท ฺฉ ูุชุฎุตุต ููุจ ููุฑุฏ ุงุฑุฒุงุจ ูุฑุงุฑ ฺฏุฑุฏ ุชุง ุงุฒ ุฎุทุฑุงุช ุงุญุชูุงู ุฌููฺฏุฑ ุดูุฏ.\n--- Example 2 ---\nQuestion: ูพุณุฑ ูู ณูุงูู ู ธุฑูุฒ ุฏุงุฑุฏ ู ูุฒู ุงู ท.ณฐฐ ุงุณุช ุดฺฉูุด ูุฑู ุฏุงุฑุฏ ู ููุด ูุฎูุงุฏ ุงูฺฏุดุช ุดุตุชุด ุจูฺฉุฏ ู ุฏุฑ ุจุงุฏ ฺฏููุด ุจุฑูู ูุงุฏ ู ุตุจุญ ูุง ุจุง ุจู ฺฏุฑูุชู ุจุฏุงุฑ ูุดู ู ููุด ูุฎูุงุฏ ุดุฑ ุจุฎูุฑู ู ฺฏุงู ุงุฒ ุดุฑ ุฎูุฑุฏู ุงูุชูุงุน ูฺฉูู ู ุณุฑ ุงู ุฎู ุฏุงุบู ู ููุด ุนุฑู ูฺฉูู ฺูุฏ ููุช ูุช ุฏุฑ ฺฉูุงุฑ ุดุฑ ุฎูุฑู ูุฑ ุงุฒ ฺฏุงู ุดุฑ ุฎุดฺฉ ุณููพุฑุงูู ูู ุจูุด ูุฏู ููฺฉูู ูุดู ุฑุงูููุงู ฺฉูุฏุ\nAnswer: ุจุฑ ุงุณุงุณ ุชูุถุญุงุช ฺฉู ุงุฑุงุฆู ุฏุงุฏุฏุ ูุดุงููโูุง ฺฉู ูุฑุฒูุฏุชุงู ูุดุงู ูโุฏูุฏ ูุงููุฏ ูฺฉุฏู ุงูฺฏุดุชุ ุจุงุฏ ฺฏูู ุฏุฑ ูุถูุ ุจู ฺฏุฑูุชู ุฏุฑ ุตุจุญโูุงุ ู ุฏูุง ุจุฏู ุจุงูุง ูโุชูุงููุฏ ูุดุงููโูุง ุงุฒ ุฑููุงฺฉุณ ูพููุงู ุจุงุดูุฏ. ุฑููุงฺฉุณ ุฏุฑ ููุฒุงุฏุงู ุฒูุงู ุฑุฎ ูโุฏูุฏ ฺฉู ุดุฑ ุง ูุฑูููุง ูุตุฑู ุจู ูุฑ ุจุฑฺฏุฑุฏุฏุ ฺฉู ูโุชูุงูุฏ ุจู ุนูุงุฆู ูุธุฑ ุชุญุฑฺฉโูพุฐุฑ ู ูุดุงููโูุง ฺฉู ุดูุง ุชูุถุญ ุฏุงุฏูโุงุฏ ููุฌุฑ ุดูุฏ. ุชุบุฑุงุช ุฏุฑ ุฑฺู ุบุฐุง ูุงุฏุฑ ุฏุฑ ุตูุฑุช ุดุฑุฏูุ ุงุณุชูุงุฏู ุงุฒ ุดุฑ ุฎุดฺฉ ูุง ฺฉู ฺฉูุชุฑ ุญุณุงุณุชโุฒุง ูุณุชูุฏุ ู ูฺฏู ุฏุงุดุชู ฺฉูุฏฺฉ ุฏุฑ ูุถุนุช ููู ูุดุณุชู ูพุณ ุงุฒ ุบุฐุง ูโุชูุงููุฏ ฺฉูฺฉโฺฉููุฏู ุจุงุดูุฏ. ููฺููุ ุชูุตู ูโุดูุฏ ุฌูุช ุงุฑุฒุงุจ ุฏููโุชุฑ ู ุฏุฑูุงูุ ุจุง ูพุฒุดฺฉ ฺฉูุฏฺฉุงู ูุดูุฑุช ููุงุฏ.\n--- Example 3 ---\nQuestion: ุจูุงุฑ ra ุฏุงุฑู ุฏฺฉุชุฑ ุจุฑุงู ูพุฑุฏูุฒููู ต ู ุณูููุงุณุงูุงุฒู ตฐฐ ุชุฌูุฒ ฺฉุฑุฏู ุจููุฑุงู ุงุณุฏ ูููฺฉ ฺฉ ููุชู ูุณุช ฺฉู ุงุฒ ุฏุงุฑููุง ุงุณุชูุงุฏู ู ฺฉูู ุงูุง ุฏู ุฑูุฒ ฺฉู ฺฉูู ุณูุช ุฑุงุณุชู ุฏุฑุฏ ูฺฉูู ู ุฎูุงุณุชู ุจุฏููู ุงุฒ ุนูุงุฑุถ ุณูููุงุณุงูุงุฒู ู ุงูฺฉู ุงฺฏู ูุณุช ุจุงุฏ ูุทุน ฺฉูู ู ุฌุงฺฏุฒู ุฏุงุฑู ฺ ุงุณุชูุงุฏู ฺฉูู ุง ูู ุงุฒ ุฏุงุฑู ุฏฺฏู ุชุง ููุชู ุงูุฏู ุจู ูพุฒุดฺฉู ุฏุฑุณุช ูุฏุงุฑู ูุทูุง ุฑุงูููุง ุจูุฑูุงูุ\nAnswer: ุฏุฑุฏ ฺฉูู ูพุณ ุงุฒ ูุตุฑู ุฏุงุฑู ุฏุฑ ุจุฑุฎ ููุงุฑุฏ ูโุชูุงูุฏ ูุดุงูโุฏููุฏู ูุงฺฉูุด ุจุฏู ุจู ุฏุงุฑู ุจุงุดุฏ. ุณูููุงุณุงูุงุฒูุ ฺฉู ุจุฑุง ุฏุฑูุงู ุฑููุงุชูุฆุฏ ุขุฑุชุฑุช (RA) ุชุฌูุฒ ูโุดูุฏุ ููฺฉู ุงุณุช ุจุงุนุซ ุงูุฒุงุด ุฎุทุฑ ุจุฑูุฒ ูุดฺฉูุงุช ฺฉูู ุดูุฏุ ุงฺฏุฑฺู ุงู ุนุงุฑุถู ูุณุจุชุง ูุงุฏุฑ ุงุณุช. ุนูุงุฆู ูุงููุฏ ุฏุฑุฏ ฺฉูู ูุงุฒููุฏ ุชูุฌู ูพุฒุดฺฉ ููุฑ ุงุณุช ุชุง ุงุฒ ุนูุงุฑุถ ุฌุฏโุชุฑ ูพุดฺฏุฑ ุดูุฏ. ููุงุณุจ ุงุณุช ฺฉู ููุฑุงู ุจุง ูพุฒุดฺฉ ุฎูุฏ ุชูุงุณ ุจฺฏุฑุฏ ู ุฏุฑ ููุฑุฏ ุนูุงุฆู ุฎูุฏ ุงุทูุงุน ุฏูุฏ. ููฺฉู ุงุณุช ูพุฒุดฺฉ ุชุตูู ุจฺฏุฑุฏ ฺฉู ุขุฒูุงุดโูุง ุจุดุชุฑ ุงูุฌุงู ุฏูุฏ ุง ุฏุงุฑู ุฌุงฺฏุฒู ุชุฌูุฒ ฺฉูุฏ. ุชูุตู ูโุดูุฏ ุชุง ุฒูุงู ูุดูุฑุช ุจุง ูพุฒุดฺฉุ ุฏุงุฑู ุฑุง ุจุฏูู ุชูุตูโ ูพุฒุดฺฉ ูุทุน ูฺฉูุฏ.\n--- Example 4 ---\nQuestion: ุณูุงู. . ุฏูููุชู ูพุด ูุจู ุงุฒ ูพุฑูุฏ ุชุง ฺูุฏ ุฑูุฒ ุฏุฑุฏ ุฏุฑ ุงู ูุงุญู ฺฉู ุชูุฏู ุง ุญุณ ูุดุฏ ุญุณ ฺฉุฑุฏู. ุณูููฺฏุฑุงู ุงูู ฺฉู ุฑูุชู ฺฏูุชู ูุดฺฉูฺฉ ูุณุช ู ุจุงุฏ ููููู ุจุฑุฏุงุฑ ุจุดู. ููู ุฑูุชู ูุงููฺฏุฑุงู ุงูุง ุฏูุจุงุฑู ฺฏูุชู ูุฌุฏุฏุง ุจุงุฏ ุณููู ุจุดู. ูุดู ุจูุฑูุงุฏ ูุดฺฉู ูุณุชุ\nAnswer: ุจุงูุช ุณูู ูุชุฑุงฺฉู ูุชุฑูฺู ุจู ุงู ูุนู ุงุณุช ฺฉู ุจุงูุช ุณูู ุดุงูู ุชุฑฺฉุจุงุช ูุชููุน ุงุฒ ุจุงูุช ฺุฑุจ ู ุจุงูุช ุบุฏุฏ-ูุจุฑ ุงุณุช ฺฉู ูโุชูุงูุฏ ุชูุณุฑ ุชุตุงูุฑ ูุงููฺฏุฑุงู ุฑุง ุฏุดูุงุฑ ุณุงุฒุฏ. ุฏุฑุฏ ู ุงุญุณุงุณ ูุฌูุฏ ุชูุฏู ูโุชูุงูุฏ ูุงุด ุงุฒ ุชุบุฑุงุช ููุฑููู ูุจู ุงุฒ ูพุฑูุฏ ุง ุณุงุฑ ุนูุงูู ุจุงุดุฏุ ุงูุง ุชูุตู ุนููู ุงู ุงุณุช ฺฉู ูุฑฺฏููู ูุดฺฉูฺฉ ุจุงุฏ ุจุง ุฏูุช ุจุฑุฑุณ ุดูุฏ. \n\t\tุฏุฑ ููุงุฑุฏ ฺฉู ุณูููฺฏุฑุงู ุง ูุงููฺฏุฑุงู ูุชุงุฌ ูุดฺฉูฺฉ ูุดุงู ูโุฏูุฏุ ุงูุฌุงู ูุฌุฏุฏ ุณูููฺฏุฑุงู ุง ูููููโุจุฑุฏุงุฑ (ุจููพุณ) ุชูุตู ูโุดูุฏ ุชุง ูุงูุช ุฏูู ุชูุฏู ุง ุชุบุฑุงุช ุจุงูุช ูุดุฎุต ุดูุฏ. ฺฏุงู ุงููุงุชุ ุฏุฑุฎูุงุณุช ุจุฑุง ุณูููฺฏุฑุงู ูุฌุฏุฏ ุจู ุฏูู ูุงุฒ ุจู ุชุตุงูุฑ ูุงุถุญโุชุฑ ุง ุงุฑุฒุงุจ ุจุดุชุฑ ุงุณุช. ููู ุงุณุช ฺฉู ุฏุณุชูุฑุงูุนููโูุง ูพุฒุดฺฉ ุฑุง ุฏููุงู ุฏูุจุงู ฺฉุฑุฏู ู ุฌูุช ฺฉุณุจ ุงุทููุงู ุจุดุชุฑ ู ุฑุฏ ูุฑฺฏููู ุจูุงุฑ ุฌุฏโุชุฑุ ูพฺฏุฑโูุง ูุงุฒู ุงูุฌุงู ุดูุฏ.\n--- Example 5 ---\nQuestion: ุณูุงู ููุช ุจุฎุฑ ุฏูุฏูู ุนููู ุณ ุณุงุนุช ูพุด ฺฉุดุฏู ูู ููฺูุงู ููุดู ฺฏุงุฒ ุงุณุชุฑู ุจุฑุฏุงุดุช ู ุฎููุฑุฒ ุฏุงุฑู ุจุงุฏ ฺฺฉุงุฑ ฺฉููุ\nAnswer: ุณูุงู ุฏูุณุช ุนุฒุฒ ุ ูุนูููุง ุฎูู ุขุจู ูุณุช ุ ุณุน ฺฉูุฏ ููุฑุช ุจุฏุฏ ู ุงุตูุง ุชู ูฺฉูุฏ . ุงฺฏุฑ ุจุนุฏ ุงุฒ ูุดุงุฑ ูุฏุงูู ฺฉ ุฏู ุณุงุนุช ุ ุฎูู ูู ุฎูู ุขุจู ุฏุฑ ุฏูุงูุชูู ูพุฑ ูุดูุ ุจู ุฏูุฏุงููพุฒุดฺฉ ูุฑุงุฌุนู ฺฉูุฏ ฺฉู ุจุฑุงุชูู ุจุฎู ุจุฒูู ุง ฺฉ ุขููพูู ุชุฑุงูฺฏุฒุงูฺฉ ุงุณุฏ ุจุฎุฑุฏ ุฑู ฺฏุงุฒ ุจุฑุฒุฏ ุจุฒุงุฑุฏ ุฑู ูุงุญู ุฎููุฑุฒ .\n\nFormatting the dataset into 'text' column...\n\nFirst 3 formatted examples:\n--- Formatted Example 1 ---\nุณูุงู: ฑูุงููพุด ุงุฒ ุฎูุงุจ ูพุฑุฏู ุงุฒ ุชุฑุณ ุดุฏุฏ ุจุนุฏุงุฒ ุงูู ุชุฑุณุูุดุงุฑู ุจุงูุง ูุฑูุช ฑด ฑต ุจุง ุณุฑ ุฏุฑุฏ ู ุฏุฑุฏ ููุณู ุณูู ููุฑุงู ุจูุฏ ุจุง ูพุฑุงููู ฺฉูุชุฑุด ูฺฉุฑุฏู ูู ุงูุงู ฒ ณ ุฑูุฒู ูุดุงุฑู ูุงุฏ ูพุงู ธ น ุฑู ณ ด ููุชุง ฺฉู ูุฑู ูพุงุฏู ุฑู ุง ฺฉูุง ูุนุงูุช ุฏุงุฑู ุจุนุฏุด ุถุฑุจุงูู ุชุง ถ ท ุณุงุนุช ุญุช ุจุดุชุฑ ุชูุฏู ู ฺฉูุฏ ู ุทุจุน ููุดู ุฎุณุชูู ุดุฏู ุฏฺฏููุชุฑุณู ุงุชูุงู ุจุฑุงู ุจููุชู ูุดุงุฑ ธ น ุฑูุฑ ด ต ุฎุทุฑูุงฺฉูุ\nูพุงุณุฎ: ุชุฌุฑุจูโุง ฺฉู ุดุฑุญ ุฏุงุฏูโุงุฏุ ูุดุงูโุฏููุฏูโ ุฑุฎุฏุงุฏูุง ูุฒูููฺฺฉ ู ุงุญุชูุงูุงู ูพุงุชูููฺฺฉ ุฏุฑ ุจุฏู ุดูุง ุงุณุช. ุชุฑุณ ุดุฏุฏ ู ูพุฑุด ุงุฒ ุฎูุงุจ ูโุชูุงูุฏ ููุฌุฑ ุจู ุงูุฒุงุด ูููุช ูุดุงุฑ ุฎูู ู ุชูพุด ููุจ ุดูุฏุ ฺฉู ุงู ูุงฺฉูุด ุทุจุน ุงุณุช. ุจุง ุงู ุญุงูุ ุงุฏุงููโ ุงู ุนูุงุฆู ู ุชุบุฑุงุช ูุดุงุฑ ุฎูู ุงุฒ ุจุงูุง ุจู ูพุงู ูุงุฒููุฏ ุจุฑุฑุณ ุจุดุชุฑ ุงุณุช. ูุดุงุฑ ุฎูู ุดูุง ฺฉู ฺฏุงู ุจู 8/9 ูโุฑุณุฏ ู ุถุฑุจุงู ููุจ ุจุงูุง ูพุณ ุงุฒ ูุนุงูุชุ ููฺฉู ุงุณุช ูุดุงููโุง ุงุฒ ุงุฎุชูุงู ุฏุฑ ุชูุธู ูุดุงุฑ ุฎูู ุง ูุดฺฉูุงุช ููุจ ุจุงุดุฏ. ูุฑุงุฌุนู ุจู ูพุฒุดฺฉุ ุงูุฌุงู ุขุฒูุงุดโูุง ุชุฎุตุต ููุจ ู ุนุฑูู ู ุดุงุฏ ูุดุงูุฑู ุจุง ฺฉ ุฑูุงููพุฒุดฺฉ ุง ุฑูุงูุดูุงุณ ุจุฑุง ูุฏุฑุช ุงุณุชุฑุณ ู ุชุฑุณ ุดุฏุฏ ูพุดููุงุฏ ูโุดูุฏ. ุงุทููุงู ุงุฒ ฺฉูุชุฑู ุฏุฑุณุช ูุดุงุฑ ุฎูู ุจุง ุฏุงุฑู ู ุจุฑูุงููโุฑุฒ ุจุฑุง ูุนุงูุช ุจุฏู ููุงุณุจ ู ูุฏุงูู ุงููุช ุฏุงุฑุฏ. ูุถุนุช ูุดุงุฑ 8/9 ุฑู 4/5 ฺฉู ุชุฌุฑุจู ูโฺฉูุฏุ ูุงุฒู ุงุณุช ฺฉู ุจู ุตูุฑุช ุฌุฏ ุชูุณุท ฺฉ ูุชุฎุตุต ููุจ ููุฑุฏ ุงุฑุฒุงุจ ูุฑุงุฑ ฺฏุฑุฏ ุชุง ุงุฒ ุฎุทุฑุงุช ุงุญุชูุงู ุฌููฺฏุฑ ุดูุฏ.\n--- Formatted Example 2 ---\nุณูุงู: ูพุณุฑ ูู ณูุงูู ู ธุฑูุฒ ุฏุงุฑุฏ ู ูุฒู ุงู ท.ณฐฐ ุงุณุช ุดฺฉูุด ูุฑู ุฏุงุฑุฏ ู ููุด ูุฎูุงุฏ ุงูฺฏุดุช ุดุตุชุด ุจูฺฉุฏ ู ุฏุฑ ุจุงุฏ ฺฏููุด ุจุฑูู ูุงุฏ ู ุตุจุญ ูุง ุจุง ุจู ฺฏุฑูุชู ุจุฏุงุฑ ูุดู ู ููุด ูุฎูุงุฏ ุดุฑ ุจุฎูุฑู ู ฺฏุงู ุงุฒ ุดุฑ ุฎูุฑุฏู ุงูุชูุงุน ูฺฉูู ู ุณุฑ ุงู ุฎู ุฏุงุบู ู ููุด ุนุฑู ูฺฉูู ฺูุฏ ููุช ูุช ุฏุฑ ฺฉูุงุฑ ุดุฑ ุฎูุฑู ูุฑ ุงุฒ ฺฏุงู ุดุฑ ุฎุดฺฉ ุณููพุฑุงูู ูู ุจูุด ูุฏู ููฺฉูู ูุดู ุฑุงูููุงู ฺฉูุฏุ\nูพุงุณุฎ: ุจุฑ ุงุณุงุณ ุชูุถุญุงุช ฺฉู ุงุฑุงุฆู ุฏุงุฏุฏุ ูุดุงููโูุง ฺฉู ูุฑุฒูุฏุชุงู ูุดุงู ูโุฏูุฏ ูุงููุฏ ูฺฉุฏู ุงูฺฏุดุชุ ุจุงุฏ ฺฏูู ุฏุฑ ูุถูุ ุจู ฺฏุฑูุชู ุฏุฑ ุตุจุญโูุงุ ู ุฏูุง ุจุฏู ุจุงูุง ูโุชูุงููุฏ ูุดุงููโูุง ุงุฒ ุฑููุงฺฉุณ ูพููุงู ุจุงุดูุฏ. ุฑููุงฺฉุณ ุฏุฑ ููุฒุงุฏุงู ุฒูุงู ุฑุฎ ูโุฏูุฏ ฺฉู ุดุฑ ุง ูุฑูููุง ูุตุฑู ุจู ูุฑ ุจุฑฺฏุฑุฏุฏุ ฺฉู ูโุชูุงูุฏ ุจู ุนูุงุฆู ูุธุฑ ุชุญุฑฺฉโูพุฐุฑ ู ูุดุงููโูุง ฺฉู ุดูุง ุชูุถุญ ุฏุงุฏูโุงุฏ ููุฌุฑ ุดูุฏ. ุชุบุฑุงุช ุฏุฑ ุฑฺู ุบุฐุง ูุงุฏุฑ ุฏุฑ ุตูุฑุช ุดุฑุฏูุ ุงุณุชูุงุฏู ุงุฒ ุดุฑ ุฎุดฺฉ ูุง ฺฉู ฺฉูุชุฑ ุญุณุงุณุชโุฒุง ูุณุชูุฏุ ู ูฺฏู ุฏุงุดุชู ฺฉูุฏฺฉ ุฏุฑ ูุถุนุช ููู ูุดุณุชู ูพุณ ุงุฒ ุบุฐุง ูโุชูุงููุฏ ฺฉูฺฉโฺฉููุฏู ุจุงุดูุฏ. ููฺููุ ุชูุตู ูโุดูุฏ ุฌูุช ุงุฑุฒุงุจ ุฏููโุชุฑ ู ุฏุฑูุงูุ ุจุง ูพุฒุดฺฉ ฺฉูุฏฺฉุงู ูุดูุฑุช ููุงุฏ.\n--- Formatted Example 3 ---\nุณูุงู: ุจูุงุฑ ra ุฏุงุฑู ุฏฺฉุชุฑ ุจุฑุงู ูพุฑุฏูุฒููู ต ู ุณูููุงุณุงูุงุฒู ตฐฐ ุชุฌูุฒ ฺฉุฑุฏู ุจููุฑุงู ุงุณุฏ ูููฺฉ ฺฉ ููุชู ูุณุช ฺฉู ุงุฒ ุฏุงุฑููุง ุงุณุชูุงุฏู ู ฺฉูู ุงูุง ุฏู ุฑูุฒ ฺฉู ฺฉูู ุณูุช ุฑุงุณุชู ุฏุฑุฏ ูฺฉูู ู ุฎูุงุณุชู ุจุฏููู ุงุฒ ุนูุงุฑุถ ุณูููุงุณุงูุงุฒู ู ุงูฺฉู ุงฺฏู ูุณุช ุจุงุฏ ูุทุน ฺฉูู ู ุฌุงฺฏุฒู ุฏุงุฑู ฺ ุงุณุชูุงุฏู ฺฉูู ุง ูู ุงุฒ ุฏุงุฑู ุฏฺฏู ุชุง ููุชู ุงูุฏู ุจู ูพุฒุดฺฉู ุฏุฑุณุช ูุฏุงุฑู ูุทูุง ุฑุงูููุง ุจูุฑูุงูุ\nูพุงุณุฎ: ุฏุฑุฏ ฺฉูู ูพุณ ุงุฒ ูุตุฑู ุฏุงุฑู ุฏุฑ ุจุฑุฎ ููุงุฑุฏ ูโุชูุงูุฏ ูุดุงูโุฏููุฏู ูุงฺฉูุด ุจุฏู ุจู ุฏุงุฑู ุจุงุดุฏ. ุณูููุงุณุงูุงุฒูุ ฺฉู ุจุฑุง ุฏุฑูุงู ุฑููุงุชูุฆุฏ ุขุฑุชุฑุช (RA) ุชุฌูุฒ ูโุดูุฏุ ููฺฉู ุงุณุช ุจุงุนุซ ุงูุฒุงุด ุฎุทุฑ ุจุฑูุฒ ูุดฺฉูุงุช ฺฉูู ุดูุฏุ ุงฺฏุฑฺู ุงู ุนุงุฑุถู ูุณุจุชุง ูุงุฏุฑ ุงุณุช. ุนูุงุฆู ูุงููุฏ ุฏุฑุฏ ฺฉูู ูุงุฒููุฏ ุชูุฌู ูพุฒุดฺฉ ููุฑ ุงุณุช ุชุง ุงุฒ ุนูุงุฑุถ ุฌุฏโุชุฑ ูพุดฺฏุฑ ุดูุฏ. ููุงุณุจ ุงุณุช ฺฉู ููุฑุงู ุจุง ูพุฒุดฺฉ ุฎูุฏ ุชูุงุณ ุจฺฏุฑุฏ ู ุฏุฑ ููุฑุฏ ุนูุงุฆู ุฎูุฏ ุงุทูุงุน ุฏูุฏ. ููฺฉู ุงุณุช ูพุฒุดฺฉ ุชุตูู ุจฺฏุฑุฏ ฺฉู ุขุฒูุงุดโูุง ุจุดุชุฑ ุงูุฌุงู ุฏูุฏ ุง ุฏุงุฑู ุฌุงฺฏุฒู ุชุฌูุฒ ฺฉูุฏ. ุชูุตู ูโุดูุฏ ุชุง ุฒูุงู ูุดูุฑุช ุจุง ูพุฒุดฺฉุ ุฏุงุฑู ุฑุง ุจุฏูู ุชูุตูโ ูพุฒุดฺฉ ูุทุน ูฺฉูุฏ.\n\nDataset preparation for fine-tuning is complete. Ready for model loading and tokenization.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"Ensuring all necessary libraries are installed...\")\n!pip install -q -U transformers peft trl bitsandbytes scipy datasets\n!pip install -q -U \"huggingface_hub[cli]\"\n!git config --global user.name \"lbehradl\"\n!git clone https://github.com/unslothai/unsloth.git\n!pip install -q -U ./unsloth\n!pip install -q -U unsloth_zoo\nprint(\"Libraries installation/update complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:13:40.584350Z","iopub.execute_input":"2025-07-21T19:13:40.584806Z","iopub.status.idle":"2025-07-21T19:14:07.171058Z","shell.execute_reply.started":"2025-07-21T19:13:40.584784Z","shell.execute_reply":"2025-07-21T19:14:07.169977Z"}},"outputs":[{"name":"stdout","text":"Ensuring all necessary libraries are installed...\n\u001b[2K   \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth-zoo 2025.7.8 requires datasets<4.0.0,>=3.4.1, but you have datasets 4.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mfatal: destination path 'unsloth' already exists and is not an empty directory.\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nLibraries installation/update complete.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import transformers \ntransformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:14:13.459833Z","iopub.execute_input":"2025-07-21T19:14:13.460536Z","iopub.status.idle":"2025-07-21T19:14:15.770686Z","shell.execute_reply.started":"2025-07-21T19:14:13.460501Z","shell.execute_reply":"2025-07-21T19:14:15.769956Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<module 'transformers' from '/usr/local/lib/python3.11/dist-packages/transformers/__init__.py'>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nimport unsloth\ntry:\n    from unsloth import FastLanguageModel\n    print(\"Unsloth detected. Using FastLanguageModel for optimized loading.\")\n    USE_UNSLOTH = True\nexcept ImportError:\n    print(\"Unsloth not found. Falling back to standard Hugging Face loading.\")\n    USE_UNSLOTH = False\nprint('imported')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:14:18.970897Z","iopub.execute_input":"2025-07-21T19:14:18.971666Z","iopub.status.idle":"2025-07-21T19:14:53.636663Z","shell.execute_reply.started":"2025-07-21T19:14:18.971637Z","shell.execute_reply":"2025-07-21T19:14:53.635855Z"}},"outputs":[{"name":"stderr","text":"2025-07-21 19:14:26.148307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753125266.457614     328 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753125266.538149     328 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/tmp/ipykernel_328/1502396434.py:5: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n\nPlease restructure your imports with 'import unsloth' at the top of your file.\n  import unsloth\n","output_type":"stream"},{"name":"stdout","text":"๐ฆฅ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n๐ฆฅ Unsloth Zoo will now patch everything to make training faster!\nUnsloth detected. Using FastLanguageModel for optimized loading.\nimported\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"model_name = \"google/medgemma-4b-it\"  # ุง \"google/medgemma-27b\"\n\n# --- ุชูุธูุงุช ฺฉูุงูุชุฒุดู (Quantization) ---\n# ุจุฑุง ฺฉุงูุด ูุตุฑู ุญุงูุธู GPUุ ูุฏู ุฑุง ุจู 4-bit ฺฉูุงูุชุฒู ู ฺฉูู (QLoRA).\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",              # ููุน ฺฉูุงูุชุฒุดู\n    bnb_4bit_compute_dtype=torch.bfloat16, # ููุน ุฏุงุฏู ุจุฑุง ูุญุงุณุจุงุช\n    bnb_4bit_use_double_quant=True,         # ฺฉูุงูุชุฒุดู ุฏูฺฏุงูู ุจุฑุง ฺฉุงูุด ุจุดุชุฑ ุญุงูุธู\n)\n\n# --- ุจุงุฑฺฏุฐุงุฑ ุชูฺฉูุงุฒุฑ ---\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# ุงุทููุงู ุงุฒ ุชูุธู pad_token ุจุฑุง ุชูฺฉูุงุฒุฑ (ููู ุจุฑุง ูุฏูโูุง decoder-only)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token  # ุงุณุชูุงุฏู ุงุฒ eos_token ุจู ุนููุงู pad_token\n\ntokenizer.padding_side = \"right\"  # ูพุฏูฺฏ ุงุฒ ุณูุช ุฑุงุณุช (ุชูุตู ุดุฏู)\n\n# --- ุจุงุฑฺฏุฐุงุฑ ูุฏู ุจุง ฺฉูุงูุชุฒุดู ---\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",         # ุชูุฒุน ูุฏู ุฑู GPU ููุฌูุฏ\n    torch_dtype=torch.bfloat16 # ุงุณุชูุงุฏู ุงุฒ bfloat16 ุจุฑุง ูุญุงุณุจุงุช\n)\n\n# ุขูุงุฏู ุณุงุฒ ูุฏู ุจุฑุง ุขููุฒุด LoRA ุจุง ฺฉูุงูุชุฒุดู 4-bit\nmodel = prepare_model_for_kbit_training(model)\n\nprint(f\"\\nModel '{model_name}' and Tokenizer loaded successfully.\")\nprint(\"\\nModel structure (first few layers):\")\nprint(model)\nprint(\"\\nTokenizer padding side set to 'right'.\")\nprint(\"Ready for LoRA configuration and training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:15:01.588565Z","iopub.execute_input":"2025-07-21T19:15:01.589270Z","iopub.status.idle":"2025-07-21T19:16:03.584101Z","shell.execute_reply.started":"2025-07-21T19:15:01.589239Z","shell.execute_reply":"2025-07-21T19:16:03.583488Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4182a0a158384b7c94b4a52a03a6c79f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a759292a2ec469b91028ccda4f8ed78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11fad63abb204e7f824c1fa1c8149463"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd05a8d2172477e85807947b7cf28e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66e6c797c286451fb50adff59bbcd03f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae281fe593b45fc90ac5480764e8e13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe0be775f1384e0988540673b50bd5f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cb5f2dfec104e529d50b1d35bd72415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de2b7da01605435eb375a575aa5e3547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c6a2ebbeda4e6283f4988e5d6eda8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c9e0cf1f1de41d9bf818cb514fd005f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a38d14e1ce6b49588e5e3abfd75a576d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30c3675ccceb493d8e14b5a751c6a5ac"}},"metadata":{}},{"name":"stdout","text":"\nModel 'google/medgemma-4b-it' and Tokenizer loaded successfully.\n\nModel structure (first few layers):\nGemma3ForConditionalGeneration(\n  (model): Gemma3Model(\n    (vision_tower): SiglipVisionModel(\n      (vision_model): SiglipVisionTransformer(\n        (embeddings): SiglipVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n          (position_embedding): Embedding(4096, 1152)\n        )\n        (encoder): SiglipEncoder(\n          (layers): ModuleList(\n            (0-26): 27 x SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n      )\n    )\n    (multi_modal_projector): Gemma3MultiModalProjector(\n      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n    )\n    (language_model): Gemma3TextModel(\n      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n      (layers): ModuleList(\n        (0-33): 34 x Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n      )\n      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n      (rotary_emb): Gemma3RotaryEmbedding()\n      (rotary_emb_local): Gemma3RotaryEmbedding()\n    )\n  )\n  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n)\n\nTokenizer padding side set to 'right'.\nReady for LoRA configuration and training.\n","output_type":"stream"}],"execution_count":7}]}