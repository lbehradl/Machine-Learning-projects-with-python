{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- ورود به حساب کاربری Hugging Face ---\n# این کد را پس از تنظیم 'HF_TOKEN' در Kaggle Secrets، در یک سلول جداگانه اجرا کنید.\nfrom huggingface_hub import login\nimport os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"HF_TOKEN\")\ntry:\n    login(token=secret_value)\n    print(\"\\nSuccessfully logged in to Hugging Face using Kaggle Secret.\")\nexcept:\n    print(\"\\nError: HF_TOKEN Kaggle Secret not found. Please ensure it's created and attached.\")\n    print(\"You can try manual login by uncommenting the line below and running again.\")\n    # login() # uncomment this line to try manual login if secret method fails\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T18:11:45.807555Z","iopub.execute_input":"2025-07-21T18:11:45.807787Z","iopub.status.idle":"2025-07-21T18:11:45.953176Z","shell.execute_reply.started":"2025-07-21T18:11:45.807766Z","shell.execute_reply":"2025-07-21T18:11:45.948286Z"}},"outputs":[{"name":"stdout","text":"\nSuccessfully logged in to Hugging Face using Kaggle Secret.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- مرحله 1: آماده سازی محیط Kaggle و نصب کتابخانه ها ---\n# این دستورات را مستقیماً در یک سلول کد در Kaggle Notebook خودتان اجرا کنید.\n\n# --- تنظیمات محیطی برای استفاده از TPU ---\nimport os\n# در محیط TPU، نیازی به تنظیم CUDA_VISIBLE_DEVICES یا PYTORCH_CUDA_ALLOC_CONF نیست.\n# TPU ها به صورت متفاوتی مدیریت می شوند.\n\n# برای جلوگیری از هشدارهای مربوط به توکن سازی موازی\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nprint(\"Environment configured for TPU usage.\")\n\n\n# نصب کتابخانه های اصلی مورد نیاز برای فاین تیونینگ LLM ها\n# transformers: کتابخانه اصلی Hugging Face برای مدل های ترانسفورمر\n# accelerate: برای آموزش بهینه و توزیع شده (مهم برای TPU)\n# peft: Parameter-Efficient Fine-Tuning (برای LoRA)\n# trl: Transformer Reinforcement Learning (شامل SFTTrainer)\n# datasets: برای بارگذاری و پردازش دیتاست ها از Hugging Face Hub\n# bitsandbytes و unsloth برای TPU استفاده نمی شوند و حذف شده اند.\n!pip install -q -U transformers accelerate peft trl scipy datasets\n# نصب کتابخانه huggingface_hub برای ابزارهای خط فرمان و لاگین\n!pip install -q -U \"huggingface_hub[cli]\"\n\nprint(\"All necessary libraries are being installed. This might take a few minutes.\")\nprint(\"Please ensure your Kaggle Notebook has a TPU accelerator enabled (e.g., TPU v3-8).\")\n\n# --- ورود به حساب کاربری Hugging Face ---\n# این مرحله برای دانلود مدل MedGemma ضروری است.\n# توکن خود را از https://huggingface.co/settings/tokens ایجاد کرده و آن را به عنوان یک Secret در Kaggle اضافه کنید.\n# نام Secret را 'HF_TOKEN' قرار دهید.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T18:11:46.993882Z","iopub.execute_input":"2025-07-21T18:11:46.994181Z","iopub.status.idle":"2025-07-21T18:11:55.767542Z","shell.execute_reply.started":"2025-07-21T18:11:46.994156Z","shell.execute_reply":"2025-07-21T18:11:55.762955Z"}},"outputs":[{"name":"stdout","text":"Environment configured for TPU usage.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nAll necessary libraries are being installed. This might take a few minutes.\nPlease ensure your Kaggle Notebook has a TPU accelerator enabled (e.g., TPU v3-8).\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- مرحله 2: بارگذاری و آماده سازی دیتاست ---\n# این کد را در یک سلول جدید در Kaggle Notebook خودتان اجرا کنید.\n# مطمئن شوید که کتابخانه 'datasets' نصب شده و به Hugging Face لاگین کرده اید.\n\nfrom datasets import load_dataset\n\nprint(\"Loading the gaokerena/MF3QA dataset from Hugging Face Hub...\")\ntry:\n    # بارگذاری دیتاست از Hugging Face Hub\n    dataset = load_dataset(\"gaokerena/MF3QA\")\n    print(\"Dataset loaded successfully.\")\n    print(\"\\nDataset structure:\")\n    print(dataset) # نمایش ساختار دیتاست (معمولاً شامل 'train', 'validation', 'test' split ها)\n    \n    # ما برای فاین تیونینگ از 'train' split استفاده می کنیم.\n    # اگر دیتاست فقط یک split داشته باشد و اسمش 'train' نباشد، باید اسم صحیح را پیدا کرد.\n    if 'train' in dataset:\n        train_dataset = dataset['train']\n    else:\n        # اگر فقط یک split وجود دارد، آن را انتخاب کن\n        train_dataset = dataset[list(dataset.keys())[0]]\n\n    print(f\"\\nNumber of examples in the training split: {len(train_dataset)}\")\n    print(\"\\nFirst 5 raw examples from the dataset:\")\n    for i in range(min(5, len(train_dataset))):\n        print(f\"--- Example {i+1} ---\")\n        # استفاده از 'Question' و 'Answer' با حرف بزرگ (مطابق با ساختار دیتاست MF3QA)\n        print(f\"Question: {train_dataset[i]['Question']}\")\n        print(f\"Answer: {train_dataset[i]['Answer']}\")\n\n    # --- فرمت بندی دیتاست برای Instruction Tuning ---\n    # مدل های LLM Instruction-tuned (مثل MedGemma) بهتر است داده ها را به فرمت مکالمه ای دریافت کنند.\n    # ما جفت های سوال و جواب را به یک رشته واحد تبدیل می کنیم که مدل بتواند آن را تکمیل کند.\n    # مثلاً \"سوال: [سوال_متن]\\nپاسخ: [پاسخ_متن]\"\n\n    def format_example(example):\n        # اطمینان حاصل می کنیم که 'Question' و 'Answer' وجود دارند و رشته هستند.\n        question = str(example.get('Question', '')).strip()\n        answer = str(example.get('Answer', '')).strip()\n\n        # فرمت استاندارد برای Instruction Tuning\n        # می توان از <|user|> و <|assistant|> هم استفاده کرد اگر مدل با آن ها آموزش دیده باشد.\n        # اما برای شروع، این فرمت ساده و واضح است.\n        formatted_text = f\"سوال: {question}\\nپاسخ: {answer}\"\n        return {\"text\": formatted_text}\n\n    print(\"\\nFormatting the dataset into 'text' column...\")\n    # متد .map() را برای اعمال تابع format_example روی کل دیتاست استفاده می کنیم.\n    # remove_columns=train_dataset.column_names: ستون های اصلی (Question, Answer, Source) را حذف کرده و فقط ستون 'text' را نگه می دارد.\n    formatted_dataset = train_dataset.map(format_example, remove_columns=train_dataset.column_names)\n    \n    print(\"\\nFirst 3 formatted examples:\")\n    for i in range(min(3, len(formatted_dataset))):\n        print(f\"--- Formatted Example {i+1} ---\")\n        print(formatted_dataset[i]['text'])\n\n    # دیتاست آماده شده را می توانیم برای مراحل بعدی (توکن سازی و آموزش) استفاده کنیم.\n    # نیازی به ذخیره به دیسک در این مرحله نیست و می توانیم آن را در حافظه نگه داریم.\n    \n    print(\"\\nDataset preparation for fine-tuning is complete. Ready for model loading and tokenization.\")\n\nexcept Exception as e:\n    print(f\"An error occurred during dataset loading or preparation: {e}\")\n    print(\"Please ensure you have successfully logged in to Hugging Face and your internet connection is stable.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T18:11:55.769563Z","iopub.execute_input":"2025-07-21T18:11:55.769780Z","iopub.status.idle":"2025-07-21T18:11:57.659762Z","shell.execute_reply.started":"2025-07-21T18:11:55.769756Z","shell.execute_reply":"2025-07-21T18:11:57.653364Z"}},"outputs":[{"name":"stdout","text":"Loading the gaokerena/MF3QA dataset from Hugging Face Hub...\nDataset loaded successfully.\n\nDataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['Question', 'Answer', 'Source'],\n        num_rows: 20000\n    })\n    dev: Dataset({\n        features: ['Question', 'Answer', 'Source'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['Question', 'Answer', 'Source'],\n        num_rows: 2000\n    })\n})\n\nNumber of examples in the training split: 20000\n\nFirst 5 raw examples from the dataset:\n--- Example 1 ---\nQuestion: ۱ماهپیش از خواب پریدم از ترس شدید بعداز اون ترس،فشارم بالا میرفت ۱۴ ۱۵ با سر درد و درد قفسه سینه همراه بود با پرانول کنترش میکردم ولی الان ۲ ۳ روزه فشارم میاد پاین ۸ ۹ روی ۳ ۴ وقتایی که میرم پیاده روی یا کلا فعالیتی دارم بعدش ضربانم تا ۶ ۷ ساعت حتی بیشتر تنده و کند و طبیعی نمیشه خستهه شدم دیگهمیترسم اتفاقی برام بیوفته فشار ۸ ۹ روری ۴ ۵ خطرناکه؟\nAnswer: تجربه‌ای که شرح داده‌اید، نشان‌دهنده‌ی رخدادهای فیزیولوژیک و احتمالاً پاتولوژیک در بدن شما است. ترس شدید و پرش از خواب می‌تواند منجر به افزایش موقت فشار خون و تپش قلب شود، که این واکنشی طبیعی است. با این حال، ادامه‌ی این علائم و تغییرات فشار خون از بالا به پایین نیازمند بررسی بیشتر است. فشار خون شما که گاهی به 8/9 می‌رسد و ضربان قلب بالا پس از فعالیت، ممکن است نشانه‌ای از اختلال در تنظیم فشار خون یا مشکلات قلبی باشد. مراجعه به پزشک، انجام آزمایش‌های تخصصی قلب و عروق و شاید مشاوره با یک روانپزشک یا روانشناس برای مدیریت استرس و ترس شدید پیشنهاد می‌شود. اطمینان از کنترل درست فشار خون با دارو و برنامه‌ریزی برای فعالیت بدنی مناسب و مداوم اهمیت دارد. وضعیت فشار 8/9 روی 4/5 که تجربه می‌کنید، لازم است که به صورت جدی توسط یک متخصص قلب مورد ارزیابی قرار گیرد تا از خطرات احتمالی جلوگیری شود.\n--- Example 2 ---\nQuestion: پسر من ۳ماهه و ۸روز دارد و وزن او ۷.۳۰۰ است شکمش ورم دارد و همش میخواد انگشت شصتش بمکد و دیر باد گلوش بیرون میاد و صبح ها با بینی گرفته بیدار میشه و همش میخواد شیر بخوره و گاهی از شیر خوردن امتناع میکنه و سر او خیلی داغه و همش عرق میکنه چند وقتی هیت در کنار شیر خورم هر از گاهی شیر خشک سوپرامیل هم بهش میدم ممکنه میشه راهنماییم کنید؟\nAnswer: بر اساس توضیحاتی که ارائه دادید، نشانه‌هایی که فرزندتان نشان می‌دهد مانند مکیدن انگشت، باد گلوی دیر هضم، بینی گرفته در صبح‌ها، و دمای بدن بالا می‌توانند نشانه‌هایی از رفلاکس پنهان باشند. رفلاکس در نوزادان زمانی رخ می‌دهد که شیر یا فرمولا مصرفی به مری برگردد، که می‌تواند به علائمی نظیر تحریک‌پذیری و نشانه‌هایی که شما توضیح داده‌اید منجر شود. تغییرات در رژیم غذایی مادر در صورت شیردهی، استفاده از شیر خشک هایی که کمتر حساسیت‌زا هستند، و نگه داشتن کودک در وضعیت نیمه نشسته پس از غذا می‌توانند کمک‌کننده باشند. همچنین، توصیه می‌شود جهت ارزیابی دقیق‌تر و درمان، با پزشک کودکان مشورت نمایید.\n--- Example 3 ---\nQuestion: بیماری ra دارم دکتر برام پردنیزلون ۵ و سولفاسالازین ۵۰۰ تجویز کرده بهمراه اسید فولیک یک هفته هست که از داروها استفاده می کنم اما دو روزی که کلیه سمت راستم درد میکنه می خواستم بدونم از عوارض سولفاسالازین و اینکه اگه هست باید قطع کنم و جایگزین دارو چی استفاده کنم یا نه از دارو دیگه تا هفته اینده به پزشکم درستی ندارم لطفا راهنمایی بفرمایین؟\nAnswer: درد کلیه پس از مصرف دارو در برخی موارد می‌تواند نشان‌دهنده واکنش بدن به دارو باشد. سولفاسالازین، که برای درمان روماتوئید آرتریت (RA) تجویز می‌شود، ممکن است باعث افزایش خطر بروز مشکلات کلیوی شود، اگرچه این عارضه نسبتا نادر است. علائمی مانند درد کلیه نیازمند توجه پزشکی فوری است تا از عوارض جدی‌تر پیشگیری شود. مناسب است که فوراً با پزشک خود تماس بگیرید و در مورد علائم خود اطلاع دهید. ممکن است پزشک تصمیم بگیرد که آزمایش‌های بیشتری انجام دهد یا داروی جایگزین تجویز کند. توصیه می‌شود تا زمان مشورت با پزشک، دارو را بدون توصیه‌ی پزشک قطع نکنید.\n--- Example 4 ---\nQuestion: سلام. . دوهفته پیش قبل از پریودی تا چند روز درد در ان ناحیه که توده ای حس میشد حس کردم. سونوگرافی اول که رفتم گفتن مشکوک هست و باید نمونه برداری بشه. منم رفتم ماموگرافی اما دوباره گفتن مجددا باید سونو بشم. میشه بفرمایید مشکلی هست؟\nAnswer: بافت سینه متراکم هتروژن به این معنی است که بافت سینه شامل ترکیبات متنوعی از بافت چربی و بافت غددی-فیبری است که می‌تواند تفسیر تصاویر ماموگرافی را دشوار سازد. درد و احساس وجود توده می‌تواند ناشی از تغییرات هورمونی قبل از پریود یا سایر عوامل باشد، اما توصیه عمومی این است که هرگونه مشکوکی باید با دقت بررسی شود. \n\t\tدر مواردی که سونوگرافی یا ماموگرافی نتایج مشکوک نشان می‌دهد، انجام مجدد سونوگرافی یا نمونه‌برداری (بیوپسی) توصیه می‌شود تا ماهیت دقیق توده یا تغییرات بافتی مشخص شود. گاهی اوقات، درخواست برای سونوگرافی مجدد به دلیل نیاز به تصاویر واضح‌تر یا ارزیابی بیشتر است. مهم است که دستورالعمل‌های پزشکی را دقیقاً دنبال کرده و جهت کسب اطمینان بیشتر و رد هرگونه بیماری جدی‌تر، پیگیری‌های لازم انجام شود.\n--- Example 5 ---\nQuestion: سلام وقت بخیر دندون عقلم س ساعت پیش کشیدم ولی همچنان نمیشه گاز استریل برداشت و خونریزی داره باید چکار کنم؟\nAnswer: سلام دوست عزیز ، معمولا خون آبه هست ، سعی کنید قورت بدید و اصلا تف نکنید . اگر بعد از فشار مداوم یکی دو ساعت ، خون نه خون آبه در دهانتون پر میشه، به دندانپزشک مراجعه کنید که براتون بخیه بزنن یا یک آمپول ترانگزامیک اسید بخرید روی گاز بریزید بزارید روی ناحیه خونریزی .\n\nFormatting the dataset into 'text' column...\n\nFirst 3 formatted examples:\n--- Formatted Example 1 ---\nسوال: ۱ماهپیش از خواب پریدم از ترس شدید بعداز اون ترس،فشارم بالا میرفت ۱۴ ۱۵ با سر درد و درد قفسه سینه همراه بود با پرانول کنترش میکردم ولی الان ۲ ۳ روزه فشارم میاد پاین ۸ ۹ روی ۳ ۴ وقتایی که میرم پیاده روی یا کلا فعالیتی دارم بعدش ضربانم تا ۶ ۷ ساعت حتی بیشتر تنده و کند و طبیعی نمیشه خستهه شدم دیگهمیترسم اتفاقی برام بیوفته فشار ۸ ۹ روری ۴ ۵ خطرناکه؟\nپاسخ: تجربه‌ای که شرح داده‌اید، نشان‌دهنده‌ی رخدادهای فیزیولوژیک و احتمالاً پاتولوژیک در بدن شما است. ترس شدید و پرش از خواب می‌تواند منجر به افزایش موقت فشار خون و تپش قلب شود، که این واکنشی طبیعی است. با این حال، ادامه‌ی این علائم و تغییرات فشار خون از بالا به پایین نیازمند بررسی بیشتر است. فشار خون شما که گاهی به 8/9 می‌رسد و ضربان قلب بالا پس از فعالیت، ممکن است نشانه‌ای از اختلال در تنظیم فشار خون یا مشکلات قلبی باشد. مراجعه به پزشک، انجام آزمایش‌های تخصصی قلب و عروق و شاید مشاوره با یک روانپزشک یا روانشناس برای مدیریت استرس و ترس شدید پیشنهاد می‌شود. اطمینان از کنترل درست فشار خون با دارو و برنامه‌ریزی برای فعالیت بدنی مناسب و مداوم اهمیت دارد. وضعیت فشار 8/9 روی 4/5 که تجربه می‌کنید، لازم است که به صورت جدی توسط یک متخصص قلب مورد ارزیابی قرار گیرد تا از خطرات احتمالی جلوگیری شود.\n--- Formatted Example 2 ---\nسوال: پسر من ۳ماهه و ۸روز دارد و وزن او ۷.۳۰۰ است شکمش ورم دارد و همش میخواد انگشت شصتش بمکد و دیر باد گلوش بیرون میاد و صبح ها با بینی گرفته بیدار میشه و همش میخواد شیر بخوره و گاهی از شیر خوردن امتناع میکنه و سر او خیلی داغه و همش عرق میکنه چند وقتی هیت در کنار شیر خورم هر از گاهی شیر خشک سوپرامیل هم بهش میدم ممکنه میشه راهنماییم کنید؟\nپاسخ: بر اساس توضیحاتی که ارائه دادید، نشانه‌هایی که فرزندتان نشان می‌دهد مانند مکیدن انگشت، باد گلوی دیر هضم، بینی گرفته در صبح‌ها، و دمای بدن بالا می‌توانند نشانه‌هایی از رفلاکس پنهان باشند. رفلاکس در نوزادان زمانی رخ می‌دهد که شیر یا فرمولا مصرفی به مری برگردد، که می‌تواند به علائمی نظیر تحریک‌پذیری و نشانه‌هایی که شما توضیح داده‌اید منجر شود. تغییرات در رژیم غذایی مادر در صورت شیردهی، استفاده از شیر خشک هایی که کمتر حساسیت‌زا هستند، و نگه داشتن کودک در وضعیت نیمه نشسته پس از غذا می‌توانند کمک‌کننده باشند. همچنین، توصیه می‌شود جهت ارزیابی دقیق‌تر و درمان، با پزشک کودکان مشورت نمایید.\n--- Formatted Example 3 ---\nسوال: بیماری ra دارم دکتر برام پردنیزلون ۵ و سولفاسالازین ۵۰۰ تجویز کرده بهمراه اسید فولیک یک هفته هست که از داروها استفاده می کنم اما دو روزی که کلیه سمت راستم درد میکنه می خواستم بدونم از عوارض سولفاسالازین و اینکه اگه هست باید قطع کنم و جایگزین دارو چی استفاده کنم یا نه از دارو دیگه تا هفته اینده به پزشکم درستی ندارم لطفا راهنمایی بفرمایین؟\nپاسخ: درد کلیه پس از مصرف دارو در برخی موارد می‌تواند نشان‌دهنده واکنش بدن به دارو باشد. سولفاسالازین، که برای درمان روماتوئید آرتریت (RA) تجویز می‌شود، ممکن است باعث افزایش خطر بروز مشکلات کلیوی شود، اگرچه این عارضه نسبتا نادر است. علائمی مانند درد کلیه نیازمند توجه پزشکی فوری است تا از عوارض جدی‌تر پیشگیری شود. مناسب است که فوراً با پزشک خود تماس بگیرید و در مورد علائم خود اطلاع دهید. ممکن است پزشک تصمیم بگیرد که آزمایش‌های بیشتری انجام دهد یا داروی جایگزین تجویز کند. توصیه می‌شود تا زمان مشورت با پزشک، دارو را بدون توصیه‌ی پزشک قطع نکنید.\n\nDataset preparation for fine-tuning is complete. Ready for model loading and tokenization.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- مرحله 3: بارگذاری مدل پایه و توکنایزر ---\n# این کد را در یک سلول جدید در Kaggle Notebook خودتان اجرا کنید.\n\n# مطمئن شوید که کتابخانه های لازم (transformers, accelerate, peft, trl, datasets) نصب شده اند.\n# همچنین مطمئن شوید که به Hugging Face لاگین کرده اید.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# در محیط TPU، نیازی به BitsAndBytesConfig و unsloth نیست، زیرا اینها عمدتاً برای بهینه سازی GPU هستند.\n# from bitsandbytes import BitsAndBytesConfig # حذف شد\n# from unsloth import FastLanguageModel # حذف شد\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n# from trl import SFTTrainer # *** اصلاح شده: SFTTrainer از اینجا حذف شد و به مرحله بعد منتقل می شود ***\nfrom datasets import Dataset # برای اطمینان از دسترسی به کلاس Dataset\n\n# در محیط TPU، USE_UNSLOTH همیشه False خواهد بود.\nUSE_UNSLOTH = False\n\n# نام مدل MedGemma که می‌خواهیم فاین تیون کنیم.\n# توصیه می‌شود با 4B شروع کنید.\nmodel_name = \"google/medgemma-4b-it\" # یا \"google/medgemma-27b\"\n\n# --- بارگذاری توکنایزر ---\nprint(f\"Loading tokenizer for model: {model_name}...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# اطمینان از تنظیم pad_token برای توکنایزر\n# این برای مدل های decoder-only مهم است.\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token # End-of-sequence token به عنوان pad_token\n\nprint(\"Tokenizer loaded successfully.\")\n\n# --- بارگذاری مدل ---\nprint(f\"Loading model: {model_name}...\")\n# برای TPU، مدل را بدون تنظیمات BitsAndBytesConfig بارگذاری می کنیم.\n# torch_dtype را روی torch.bfloat16 یا torch.float32 تنظیم می کنیم.\n# bfloat16 برای TPU ها بهینه است و توصیه می شود.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16, # استفاده از bfloat16 برای TPU ها\n    device_map=\"auto\", # برای توزیع مدل روی دستگاه های موجود (TPU cores)\n)\nprint(\"Model loaded successfully.\")\n\n# --- تنظیمات توکنایزر برای آموزش ---\n# این تنظیمات برای اطمینان از اینکه توکنایزر به درستی برای آموزش آماده است، ضروری است.\ntokenizer.padding_side = \"right\" # پدینگ از سمت راست (برای مدل های decoder-only توصیه می شود)\n\nprint(f\"\\nModel '{model_name}' and Tokenizer loaded successfully for TPU.\")\nprint(\"\\nModel structure (first few layers):\")\nprint(model) # نمایش ساختار مدل\n\nprint(\"\\nTokenizer padding side set to 'right'.\")\nprint(\"Ready for LoRA configuration and Trainer setup.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T18:11:57.662535Z","iopub.execute_input":"2025-07-21T18:11:57.662941Z","iopub.status.idle":"2025-07-21T18:12:11.168577Z","shell.execute_reply.started":"2025-07-21T18:11:57.662915Z","shell.execute_reply":"2025-07-21T18:12:11.160352Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1753121524.684954    5177 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:230\n","output_type":"stream"},{"name":"stdout","text":"Loading tokenizer for model: google/medgemma-4b-it...\nTokenizer loaded successfully.\nLoading model: google/medgemma-4b-it...\n","output_type":"stream"},{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully.\n\nModel 'google/medgemma-4b-it' and Tokenizer loaded successfully for TPU.\n\nModel structure (first few layers):\nGemma3ForConditionalGeneration(\n  (model): Gemma3Model(\n    (vision_tower): SiglipVisionModel(\n      (vision_model): SiglipVisionTransformer(\n        (embeddings): SiglipVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n          (position_embedding): Embedding(4096, 1152)\n        )\n        (encoder): SiglipEncoder(\n          (layers): ModuleList(\n            (0-26): 27 x SiglipEncoderLayer(\n              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (self_attn): SiglipAttention(\n                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              )\n              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): PytorchGELUTanh()\n                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n              )\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n      )\n    )\n    (multi_modal_projector): Gemma3MultiModalProjector(\n      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n    )\n    (language_model): Gemma3TextModel(\n      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n      (layers): ModuleList(\n        (0-33): 34 x Gemma3DecoderLayer(\n          (self_attn): Gemma3Attention(\n            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          )\n          (mlp): Gemma3MLP(\n            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n        )\n      )\n      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n      (rotary_emb): Gemma3RotaryEmbedding()\n      (rotary_emb_local): Gemma3RotaryEmbedding()\n    )\n  )\n  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n)\n\nTokenizer padding side set to 'right'.\nReady for LoRA configuration and Trainer setup.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- مرحله 4: پیکربندی LoRA و Trainer ---\n# این کد را در یک سلول جدید در Kaggle Notebook خودتان اجرا کنید.\n\n# مطمئن شوید که 'model' و 'tokenizer' از مرحله قبل در دسترس هستند.\n# همچنین 'formatted_dataset' که در مرحله 2 آماده کردیم.\n\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments # TrainingArguments همچنان برای برخی لاگ ها و سازگاری لازم است، اما پارامترهای آن به SFTConfig منتقل می شود\nfrom trl import SFTTrainer, SFTConfig # *** SFTConfig اضافه شد ***\n\nprint(\"Configuring LoRA and Training Arguments for TPU...\")\n\n# --- 4.1: پیکربندی LoRA (Low-Rank Adaptation) ---\n# LoRA به ما اجازه می دهد تا مدل را با آموزش فقط بخش کوچکی از وزن های آن فاین تیون کنیم.\n# این کار مصرف حافظه و زمان آموزش را به شدت کاهش می دهد.\nlora_config = LoraConfig(\n    r=16, # r (rank): ابعاد ماتریس های LoRA. مقادیر رایج 8، 16، 32، 64 هستند.\n          # مقدار بالاتر = قدرت بیانی بیشتر اما مصرف حافظه و زمان بیشتر. 16 یک نقطه شروع خوب است.\n    lora_alpha=16, # lora_alpha: یک فاکتور مقیاس گذاری برای وزن های LoRA. معمولاً برابر با r یا دو برابر آن تنظیم می شود.\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n          # target_modules: لایه هایی در مدل که LoRA روی آنها اعمال می شود.\n          # اینها معمولاً لایه های خطی در بلوک های ترانسفورمر هستند که در Gemma/MedGemma رایج هستند.\n    lora_dropout=0.05, # lora_dropout: نرخ دراپ اوت برای لایه های LoRA برای جلوگیری از Overfitting.\n    bias=\"none\", # bias: نحوه مدیریت بایاس در لایه های LoRA. \"none\" معمولاً برای LLM ها خوب است.\n    task_type=\"CAUSAL_LM\", # task_type: نوع وظیفه مدل. CAUSAL_LM برای مدل های تولید متن (مانند Gemma/MedGemma) است.\n)\n\n# --- اعمال پیکربندی LoRA به مدل ---\n# برای TPU، ما به صورت دستی آداپتور LoRA را به مدل اضافه می کنیم.\nmodel = get_peft_model(model, lora_config)\n\nprint(\"LoRA adapters attached to the model.\")\n\n# --- 4.2: ایجاد SFTConfig (جایگزین TrainingArguments برای SFTTrainer) ---\n# SFTConfig تمام پارامترهای آموزش و پارامترهای خاص SFTTrainer را در یک شیء واحد نگه می دارد.\nsft_config = SFTConfig(\n    output_dir=\"./results\", # دایرکتوری برای ذخیره لاگ ها، چک پوینت ها و مدل نهایی\n    num_train_epochs=5, # تعداد اپوک ها (دوره ها) برای آموزش. 3-5 اپوک برای شروع خوب است.\n    per_device_train_batch_size=8, # بچ سایز برای TPU معمولاً می تواند بزرگتر باشد (با 8 هسته TPU).\n                                   # اگر OOM گرفتید، آن را کاهش دهید (مثلاً 4).\n    gradient_accumulation_steps=1, # در TPU با بچ سایز بزرگتر، معمولاً نیازی به accumulation نیست.\n    optim=\"adamw_torch\", # بهینه ساز (optimizer) مورد استفاده. adamw_torch برای TPU ها مناسب است.\n    save_steps=500, # تعداد گام ها (steps) برای ذخیره چک پوینت مدل.\n    logging_steps=50, # تعداد گام ها برای لاگ کردن اطلاعات آموزش (loss و غیره).\n    learning_rate=2e-4, # learning_rate: نرخ یادگیری برای بهینه ساز.\n    fp16=False, # در TPU از bfloat16 استفاده می کنیم، پس fp16 را False نگه می داریم.\n    bf16=True, # *** فعال کردن bfloat16 برای TPU ***\n    max_grad_norm=0.3, # حداکثر نرم گرادیان برای جلوگیری از انفجار گرادیان.\n    warmup_ratio=0.03, # نسبت گام های گرم کردن (warmup) در ابتدای آموزش.\n    lr_scheduler_type=\"constant\", # نوع زمانبندی نرخ یادگیری (learning rate scheduler).\n    report_to=\"tensorboard\", # ابزاری برای گزارش دهی (می توانید \"none\" یا \"wandb\" را هم استفاده کنید).\n    disable_tqdm=False, # غیرفعال کردن نوار پیشرفت tqdm در طول آموزش (اختیاری).\n    \n    # پارامترهای خاص SFTTrainer که اکنون در SFTConfig قرار می گیرند:\n    max_seq_length=2048, # حداکثر طول دنباله. این باید با max_seq_length توکنایزر شما مطابقت داشته باشد.\n    dataset_text_field=\"text\", # نام ستونی در دیتاست که متن فرمت شده برای آموزش را شامل می شود.\n    packing=False, # برای ترکیب چندین نمونه کوتاه در یک دنباله طولانی تر.\n                   # برای دیتاست Q&A شما که پاسخ ها کوتاه هستند، ممکن است مفید باشد.\n                   # اما برای شروع False نگه می داریم تا پیچیدگی کمتر شود.\n    # اگر می خواهید مدل را در Hugging Face Hub آپلود کنید، این پارامترها را در SFTConfig فعال کنید:\n    # push_to_hub=True,\n    # hub_model_id=\"your-username/medgemma-fa-medical-qa\", # نام مخزن در Hugging Face Hub\n    # hub_private_repo=False, # اگر مخزن خصوصی است، True کنید.\n    # hub_strategy=\"every_save\", # هر بار که مدل ذخیره می شود، به هاب هم push شود.\n)\n\n# --- 4.3: آماده سازی SFTTrainer ---\n# SFTTrainer (Supervised Fine-Tuning Trainer) از کتابخانه trl برای فاین تیونینگ مدل های زبان استفاده می شود.\n# این Trainer تمامی مراحل آموزش را مدیریت می کند.\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=formatted_dataset,\n    peft_config=lora_config,\n    args=sft_config,\n    processing_class=tokenizer  # ← جایگزین `tokenizer=...`\n)\n\n\nprint(\"\\nLoRA and Training Arguments configured successfully.\")\nprint(\"SFTTrainer initialized. Ready to start training!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T18:12:13.756270Z","iopub.execute_input":"2025-07-21T18:12:13.757056Z"}},"outputs":[{"name":"stdout","text":"Configuring LoRA and Training Arguments for TPU...\n","output_type":"stream"},{"name":"stderr","text":"WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n","output_type":"stream"},{"name":"stdout","text":"LoRA adapters attached to the model.\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1753121535.677118    5177 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:232\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import trl\nprint(f\"trl version: {trl.__version__}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git config --global user.name \"lbehradl\"\n!git clone https://github.com/unslothai/unsloth.git\n!pip install -q -U ./unsloth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U unsloth_zoo\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unsloth\ntry:\n    from unsloth import FastLanguageModel\n    print(\"Unsloth detected. Using FastLanguageModel for optimized loading.\")\n    USE_UNSLOTH = True\nexcept ImportError:\n    print(\"Unsloth not found. Falling back to standard Hugging Face loading.\")\n    USE_UNSLOTH = False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- مرحله 3: بارگذاری مدل پایه و توکنایزر ---\n# این کد را در یک سلول جدید در Kaggle Notebook خودتان اجرا کنید.\n\n# --- نصب مجدد کتابخانه های مورد نیاز برای اطمینان از وجود 'trl' و سایرین ---\n# این دستورات در ابتدای این سلول قرار داده شده اند تا اطمینان حاصل شود که همه پیش نیازها قبل از import فراهم هستند.\nprint(\"Ensuring all necessary libraries are installed...\")\n!pip install -q -U transformers accelerate peft trl bitsandbytes scipy datasets\n!pip install -q -U \"huggingface_hub[cli]\"\n!pip install -q -U git+https://github.com/unsloth/unsloth.git\nprint(\"Libraries installation/update complete.\")\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nfrom datasets import Dataset # برای اطمینان از دسترسی به کلاس Dataset\n\n# اگر از Unsloth استفاده می‌کنید، بهتر است از توابع بارگذاری آن استفاده کنید.\n# این کار بهینه‌سازی‌های خاص Unsloth را فعال می‌کند.\ntry:\n    from unsloth import FastLanguageModel\n    print(\"Unsloth detected. Using FastLanguageModel for optimized loading.\")\n    USE_UNSLOTH = True\nexcept ImportError:\n    print(\"Unsloth not found. Falling back to standard Hugging Face loading.\")\n    USE_UNSLOTH = False\n\n# نام مدل MedGemma که می‌خواهیم فاین تیون کنیم.\n# توصیه می‌شود با 4B شروع کنید مگر اینکه GPU قدرتمند (A100) داشته باشید.\nmodel_name = \"google/medgemma-4b-it\" # یا \"google/medgemma-27b\"\n\n# --- تنظیمات کوانتیزیشن (Quantization) ---\n# برای کاهش مصرف حافظه GPU، مدل را به 4-bit کوانتیزه می کنیم (QLoRA).\n# این کار به شما امکان می دهد مدل های بزرگتر را روی GPU های با RAM کمتر اجرا کنید.\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\", # نوع کوانتیزیشن\n    bnb_4bit_compute_dtype=torch.bfloat16, # نوع داده برای محاسبات\n    bnb_4bit_use_double_quant=True, # استفاده از کوانتیزیشن دوگانه برای کاهش بیشتر حافظه\n)\n\n# --- بارگذاری مدل و توکنایزر ---\n# اگر Unsloth در دسترس باشد، از آن برای بارگذاری بهینه استفاده می‌کنیم.\nif USE_UNSLOTH:\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=model_name,\n        max_seq_length=2048, # حداکثر طول دنباله (sequence length) برای ورودی های مدل\n                               # این مقدار را می توانید بر اساس طول سوالات و پاسخ های خود تنظیم کنید.\n                               # 2048 یک مقدار رایج و مناسب برای شروع است.\n        dtype=None, # None به unsloth اجازه می دهد بهترین dtype را انتخاب کند (معمولا bfloat16)\n        load_in_4bit=True, # فعال کردن کوانتیزیشن 4-bit\n    )\nelse:\n    # بارگذاری توکنایزر\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    # اطمینان از تنظیم pad_token برای توکنایزر\n    # این برای مدل های decoder-only مهم است.\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token # End-of-sequence token به عنوان pad_token\n\n    # بارگذاری مدل با تنظیمات کوانتیزیشن\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\", # برای توزیع مدل روی GPU های موجود\n        torch_dtype=torch.bfloat16, # استفاده از bfloat16 برای محاسبات\n    )\n    # آماده سازی مدل برای آموزش LoRA با کوانتیزیشن 4-bit\n    model = prepare_model_for_kbit_training(model)\n\nprint(f\"\\nModel '{model_name}' and Tokenizer loaded successfully.\")\nprint(\"\\nModel structure (first few layers):\")\nprint(model) # نمایش ساختار مدل\n\n# --- تنظیمات توکنایزر برای آموزش ---\n# این تنظیمات برای اطمینان از اینکه توکنایزر به درستی برای آموزش آماده است، ضروری است.\ntokenizer.padding_side = \"right\" # پدینگ از سمت راست (برای مدل های decoder-only توصیه می شود)\n\nprint(\"\\nTokenizer padding side set to 'right'.\")\nprint(\"Ready for LoRA configuration and training.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- مرحله 4: پیکربندی LoRA و Trainer ---\n# این کد را در یک سلول جدید در Kaggle Notebook خودتان اجرا کنید.\n\n# مطمئن شوید که 'model' و 'tokenizer' از مرحله قبل در دسترس هستند.\n# همچنین 'formatted_dataset' که در مرحله 2 آماده کردیم.\n\nfrom peft import LoraConfig, get_peft_model # get_peft_model اضافه شد\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\nprint(\"Configuring LoRA and Training Arguments...\")\n\n# --- 4.1: پیکربندی LoRA (Low-Rank Adaptation) ---\n# LoRA به ما اجازه می دهد تا مدل را با آموزش فقط بخش کوچکی از وزن های آن فاین تیون کنیم.\n# این کار مصرف حافظه و زمان آموزش را به شدت کاهش می دهد.\nlora_config = LoraConfig(\n    r=16, # r (rank): ابعاد ماتریس های LoRA. مقادیر رایج 8، 16، 32، 64 هستند.\n          # مقدار بالاتر = قدرت بیانی بیشتر اما مصرف حافظه و زمان بیشتر. 16 یک نقطه شروع خوب است.\n    lora_alpha=16, # lora_alpha: یک فاکتور مقیاس گذاری برای وزن های LoRA. معمولاً برابر با r یا دو برابر آن تنظیم می شود.\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n          # target_modules: لایه هایی در مدل که LoRA روی آنها اعمال می شود.\n          # اینها معمولاً لایه های خطی در بلوک های ترانسفورمر هستند که در Gemma/MedGemma رایج هستند.\n    lora_dropout=0.05, # lora_dropout: نرخ دراپ اوت برای لایه های LoRA برای جلوگیری از Overfitting.\n    bias=\"none\", # bias: نحوه مدیریت بایاس در لایه های LoRA. \"none\" معمولاً برای LLM ها خوب است.\n    task_type=\"CAUSAL_LM\", # task_type: نوع وظیفه مدل. CAUSAL_LM برای مدل های تولید متن (مانند Gemma/MedGemma) است.\n)\n\n# --- اضافه شده: اعمال پیکربندی LoRA به مدل ---\n# این مرحله مدل پایه را با لایه های LoRA \"تقویت\" می کند و آن را برای فاین تیونینگ آماده می سازد.\n# این کار باید قبل از پاس دادن مدل به SFTTrainer انجام شود.\nmodel.add_adapter(lora_config) # اگر از Unsloth استفاده می کنید، FastLanguageModel.from_pretrained\n                               # ممکن است این مرحله را به صورت خودکار انجام دهد.\n                               # اما برای اطمینان و سازگاری با حالت استاندارد، این خط اضافه می شود.\n                               # اگر Unsloth از قبل آداپتور را اضافه کرده باشد، این خط ممکن است\n                               # هشدار دهد یا به سادگی کاری انجام ندهد.\n# model = get_peft_model(model, lora_config) # این خط برای حالت استاندارد Hugging Face است.\n                                           # Unsloth معمولا این کار را در FastLanguageModel.from_pretrained انجام می دهد.\n                                           # اگر از Unsloth استفاده می کنید، نیازی به فراخوانی get_peft_model نیست.\n                                           # اگر Unsloth را حذف کردید، این خط را فعال کنید.\n\n# --- 4.2: تنظیمات آموزش (TrainingArguments) ---\n# این تنظیمات رفتار Trainer را در طول آموزش کنترل می کند.\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\", # دایرکتوری برای ذخیره لاگ ها، چک پوینت ها و مدل نهایی\n    num_train_epochs=3, # تعداد اپوک ها (دوره ها) برای آموزش. 3-5 اپوک برای شروع خوب است.\n                        # با توجه به 15 هزار نمونه شما و 24 هزار نمونه دیتاست MF3QA، این عدد مناسب است.\n    per_device_train_batch_size=4, # تعداد نمونه در هر بچ (batch) برای هر GPU.\n                                   # این مقدار به RAM GPU شما بستگی دارد. اگر OOM گرفتید، آن را کاهش دهید (مثلاً 2).\n    gradient_accumulation_steps=2, # تعداد گام های انباشت گرادیان.\n                                   # گرادیان ها را در چندین بچ کوچک جمع می کند تا به یک بچ بزرگتر برسد.\n                                   # این به شبیه سازی batch size بزرگتر کمک می کند بدون مصرف زیاد RAM.\n                                   # (مثلاً با batch_size=4 و gradient_accumulation_steps=2، batch size موثر 8 می شود)\n    optim=\"paged_adamw_8bit\", # بهینه ساز (optimizer) مورد استفاده. paged_adamw_8bit برای QLoRA بهینه است.\n    save_steps=500, # تعداد گام ها (steps) برای ذخیره چک پوینت مدل.\n    logging_steps=50, # تعداد گام ها برای لاگ کردن اطلاعات آموزش (loss و غیره).\n    learning_rate=2e-4, # learning_rate: نرخ یادگیری برای بهینه ساز.\n    fp16=False, # *** اصلاح شده: استفاده از float32 (با تنظیم هر دو fp16 و bf16 به False) ***\n    bf16=False, # *** اصلاح شده: استفاده از float32 (با تنظیم هر دو fp16 و bf16 به False) ***\n    max_grad_norm=0.3, # حداکثر نرم گرادیان برای جلوگیری از انفجار گرادیان.\n    warmup_ratio=0.03, # نسبت گام های گرم کردن (warmup) در ابتدای آموزش.\n    lr_scheduler_type=\"constant\", # نوع زمانبندی نرخ یادگیری (learning rate scheduler).\n    report_to=\"tensorboard\", # ابزاری برای گزارش دهی (می توانید \"none\" یا \"wandb\" را هم استفاده کنید).\n    disable_tqdm=True, # غیرفعال کردن نوار پیشرفت tqdm در طول آموزش (اختیاری).\n    # اگر می خواهید مدل را در Hugging Face Hub آپلود کنید، این پارامترها را فعال کنید:\n    # push_to_hub=True,\n    # hub_model_id=\"your-username/medgemma-fa-medical-qa\", # نام مخزن در Hugging Face Hub\n    # hub_private_repo=False, # اگر مخزن خصوصی است، True کنید.\n    # hub_strategy=\"every_save\", # هر بار که مدل ذخیره می شود، به هاب هم push شود.\n)\n\n# --- 4.3: آماده سازی SFTTrainer ---\n# SFTTrainer (Supervised Fine-Tuning Trainer) از کتابخانه trl برای فاین تیونینگ مدل های زبان استفاده می شود.\n# این Trainer تمامی مراحل آموزش را مدیریت می کند.\ntrainer = SFTTrainer(\n    model=model, # مدل MedGemma که بارگذاری کردیم (حالا با آداپتورهای LoRA متصل شده)\n    train_dataset=formatted_dataset, # دیتاست آماده شده فارسی\n    peft_config=lora_config, # پیکربندی LoRA\n    tokenizer=tokenizer, # توکنایزر مدل\n    args=training_arguments, # تنظیمات آموزش\n    packing=False, # packing: برای ترکیب چندین نمونه کوتاه در یک دنباله طولانی تر.\n                   # برای دیتاست Q&A شما که پاسخ ها کوتاه هستند، ممکن است مفید باشد.\n                   # اما برای شروع False نگه می داریم تا پیچیدگی کمتر شود.\n    dataset_text_field=\"text\", # نام ستونی در دیتاست که متن فرمت شده برای آموزش را شامل می شود.\n)\n\nprint(\"\\nLoRA and Training Arguments configured successfully.\")\nprint(\"SFTTrainer initialized. Ready to start training!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- مرحله 4: پیکربندی LoRA و Trainer ---\n# این کد را در یک سلول جدید در Kaggle Notebook خودتان اجرا کنید.\n\n# مطمئن شوید که 'model' و 'tokenizer' از مرحله قبل در دسترس هستند.\n# همچنین 'formatted_dataset' که در مرحله 2 آماده کردیم.\n\nfrom peft import LoraConfig, get_peft_model # get_peft_model اضافه شد\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\nprint(\"Configuring LoRA and Training Arguments...\")\n\n# --- 4.1: پیکربندی LoRA (Low-Rank Adaptation) ---\n# LoRA به ما اجازه می دهد تا مدل را با آموزش فقط بخش کوچکی از وزن های آن فاین تیون کنیم.\n# این کار مصرف حافظه و زمان آموزش را به شدت کاهش می دهد.\nlora_config = LoraConfig(\n    r=16, # r (rank): ابعاد ماتریس های LoRA. مقادیم رایج 8، 16، 32، 64 هستند.\n          # مقدار بالاتر = قدرت بیانی بیشتر اما مصرف حافظه و زمان بیشتر. 16 یک نقطه شروع خوب است.\n    lora_alpha=16, # lora_alpha: یک فاکتور مقیاس گذاری برای وزن های LoRA. معمولاً برابر با r یا دو برابر آن تنظیم می شود.\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n          # target_modules: لایه هایی در مدل که LoRA روی آنها اعمال می شود.\n          # اینها معمولاً لایه های خطی در بلوک های ترانسفورمر هستند که در Gemma/MedGemma رایج هستند.\n    lora_dropout=0.05, # lora_dropout: نرخ دراپ اوت برای لایه های LoRA برای جلوگیری از Overfitting.\n    bias=\"none\", # bias: نحوه مدیریت بایاس در لایه های LoRA. \"none\" معمولاً برای LLM ها خوب است.\n    task_type=\"CAUSAL_LM\", # task_type: نوع وظیفه مدل. CAUSAL_LM برای مدل های تولید متن (مانند Gemma/MedGemma) است.\n)\n\n# --- اصلاح شده: اعمال پیکربندی LoRA به مدل (خط model.add_adapter حذف شد) ---\n# Unsloth (FastLanguageModel.from_pretrained) به صورت خودکار آداپتور LoRA را اضافه می کند.\n# بنابراین نیازی به فراخوانی دستی model.add_adapter یا get_peft_model نیست.\n\n\n# --- 4.2: تنظیمات آموزش (TrainingArguments) ---\n# این تنظیمات رفتار Trainer را در طول آموزش کنترل می کند.\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\", # دایرکتوری برای ذخیره لاگ ها، چک پوینت ها و مدل نهایی\n    num_train_epochs=5, # تعداد اپوک ها (دوره ها) برای آموزش. 3-5 اپوک برای شروع خوب است.\n                        # با توجه به 15 هزار نمونه شما و 24 هزار نمونه دیتاست MF3QA، این عدد مناسب است.\n    per_device_train_batch_size=2, # *** اصلاح شده: کاهش بچ سایز برای کاهش مصرف حافظه GPU ***\n    gradient_accumulation_steps=4, # *** اصلاح شده: افزایش گام های انباشت گرادیان برای حفظ effective batch size ***\n                                   # (با بچ سایز 2 و انباشت 4، effective batch size همچنان 8 است)\n    optim=\"paged_adamw_8bit\", # بهینه ساز (optimizer) مورد استفاده. paged_adamw_8bit برای QLoRA بهینه است.\n    save_steps=500, # تعداد گام ها (steps) برای ذخیره چک پوینت مدل.\n    logging_steps=50, # تعداد گام ها برای لاگ کردن اطلاعات آموزش (loss و غیره).\n    learning_rate=2e-4, # learning_rate: نرخ یادگیری برای بهینه ساز.\n    fp16=False, # استفاده از float32 (با تنظیم هر دو fp16 و bf16 به False)\n    bf16=False, # استفاده از float32 (با تنظیم هر دو fp16 و bf16 به False)\n    max_grad_norm=0.3, # حداکثر نرم گرادیان برای جلوگیری از انفجار گرادیان.\n    warmup_ratio=0.03, # نسبت گام های گرم کردن (warmup) در ابتدای آموزش.\n    lr_scheduler_type=\"constant\", # نوع زمانبندی نرخ یادگیری (learning rate scheduler).\n    report_to=\"tensorboard\", # ابزاری برای گزارش دهی (می توانید \"none\" یا \"wandb\" را هم استفاده کنید).\n    disable_tqdm=True, # غیرفعال کردن نوار پیشرفت tqdm در طول آموزش (اختیاری).\n    # اگر می خواهید مدل را در Hugging Face Hub آپلود کنید، این پارامترها را فعال کنید:\n    # push_to_hub=True,\n    # hub_model_id=\"your-username/medgemma-fa-medical-qa\", # نام مخزن در Hugging Face Hub\n    # hub_private_repo=False, # اگر مخزن خصوصی است، True کنید.\n    # hub_strategy=\"every_save\", # هر بار که مدل ذخیره می شود، به هاب هم push شود.\n)\n\n# --- 4.3: آماده سازی SFTTrainer ---\n# SFTTrainer (Supervised Fine-Tuning Trainer) از کتابخانه trl برای فاین تیونینگ مدل های زبان استفاده می شود.\n# این Trainer تمامی مراحل آموزش را مدیریت می کند.\ntrainer = SFTTrainer(\n    model=model, # مدل MedGemma که بارگذاری کردیم (حالا با آداپتورهای LoRA متصل شده)\n    train_dataset=formatted_dataset, # دیتاست آماده شده فارسی\n    peft_config=lora_config, # پیکربندی LoRA\n    tokenizer=tokenizer, # توکنایزر مدل\n    args=training_arguments, # تنظیمات آموزش\n    packing=False, # packing: برای ترکیب چندین نمونه کوتاه در یک دنباله طولانی تر.\n                   # برای دیتاست Q&A شما که پاسخ ها کوتاه هستند، ممکن است مفید باشد.\n                   # اما برای شروع False نگه می داریم تا پیچیدگی کمتر شود.\n    dataset_text_field=\"text\", # نام ستونی در دیتاست که متن فرمت شده برای آموزش را شامل می شود.\n)\n\nprint(\"\\nLoRA and Training Arguments configured successfully.\")\nprint(\"SFTTrainer initialized. Ready to start training!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}